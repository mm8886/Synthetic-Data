from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.ml import Pipeline
from pyspark.ml.feature import StandardScaler, VectorAssembler, StringIndexer, OneHotEncoder, Imputer
from pyspark.ml.base import Transformer
from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters
from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable
import re
from datetime import datetime

class DateFeatureExtractor(Transformer):
    """Extract features from date columns"""
    def __init__(self, date_columns):
        super(DateFeatureExtractor, self).__init__()
        self.date_columns = date_columns

    def _transform(self, df):
        result_df = df
        for col in self.date_columns:
            if col in df.columns:
                result_df = (result_df
                    .withColumn(f"{col}_month", month(to_date(col, "dd-MM-yyyy")))
                    .withColumn(f"{col}_dayofweek", dayofweek(to_date(col, "dd-MM-yyyy")))
                    .withColumn(f"{col}_year", year(to_date(col, "dd-MM-yyyy")))
                    .drop(col)
                )
        return result_df

class IncomeBandEncoder(Transformer):
    """Encode income bands to numerical values"""
    def __init__(self):
        super(IncomeBandEncoder, self).__init__()
        self.income_mapping = {
            '50,000 or Below': 0,
            '50,000 to 100,000': 1,
            '100,000 to 200,000': 2,
            '200,000 to 300,000': 3,
            '300,000 to 500,000': 4,
            '500,000 or Above': 5
        }

    def _transform(self, df):
        if 'Income_Band_SGD' in df.columns:
            # Create mapping expression correctly
            mapping_expr = create_map([lit(x) for item in self.income_mapping.items() for x in item])
            return df.withColumn('Income_Band_SGD', mapping_expr[col('Income_Band_SGD')])
        return df

class BooleanConverter(Transformer):
    """Convert various boolean representations to proper booleans"""
    def _transform(self, df):
        bool_columns = ['Partial_Payment_Indicator', 'Repayment_Irregularity_Flags',
                       'Mobile_Number_Active_Status', 'Email_Activity', 'Do_Not_Call_Registry_Data']
        
        result_df = df
        for col_name in bool_columns:
            if col_name in df.columns:
                result_df = result_df.withColumn(
                    col_name,
                    when(lower(col(col_name)).isin(['true', '1', 'yes']), True)
                    .when(lower(col(col_name)).isin(['false', '0', 'no']), False)
                    .otherwise(col(col_name))  # Keep original value if not in mapping
                )
        return result_df

class DigitalEngagementCalculator(Transformer):
    """Calculate digital engagement score"""
    def _transform(self, df):
        engagement_metrics = ['App_Login_Frequency', 'UPI_Transactions', 'Online_Banking_Activity']
        if all(metric in df.columns for metric in engagement_metrics):
            return df.withColumn(
                'Digital_Engagement_Score',
                (col('App_Login_Frequency') + col('UPI_Transactions') + col('Online_Banking_Activity')) / 3
            )
        return df

class AddressProcessor(Transformer):
    """Extract city and pincode from address column"""
    
    def __init__(self):
        super(AddressProcessor, self).__init__()
        self.singapore_regions = [
            'Tengah', 'Loyang', 'Pasir Ris', 'Tampines', 'Clementi', 'Jurong',
            'Queenstown', 'Woodlands', 'Serangoon', 'Bedok', 'Ang Mo Kio',
            'Toa Payoh', 'Bishan', 'Bukit Merah', 'Bukit Timah', 'Geylang',
            'Kallang', 'Marine Parade', 'Novena', 'Potong Pasir', 'Punggol',
            'Sembawang', 'Sengkang', 'Hougang', 'Yishun', 'Lim Chu Kang',
            'Mandai', 'Sungei Kadut', 'Central Area', 'Downtown Core',
            'Newton', 'Orchard', 'River Valley', 'Rochor', 'Singapore River',
            'Southern Islands', 'Straits View', 'Outram', 'Museum',
            'Dhoby Ghaut', 'Marina South', 'Marina East', 'Marina Centre'
        ]

    def _extract_city_udf(self):
        def extract_city(address):
            if address is None:
                return None
            
            address_lower = address.lower()
            for region in self.singapore_regions:
                if region.lower() in address_lower:
                    return region
            
            match = re.search(r'(\w+)\s+Singapore', address, re.IGNORECASE)
            if match:
                return match.group(1)
            
            return 'Unknown'
        return udf(extract_city, StringType())

    def _extract_pincode_udf(self):
        def extract_pincode(address):
            if address is None:
                return None
            pincode_pattern = r'\b(\d{6})\b'
            match = re.search(pincode_pattern, address)
            if match:
                return match.group(1)
            return None
        return udf(extract_pincode, StringType())

    def _transform(self, df):
        if 'Address' in df.columns:
            return (df
                .withColumn('City', self._extract_city_udf()(col('Address')))
                .withColumn('Pincode', self._extract_pincode_udf()(col('Address')))
                .drop('Address')
            )
        return df

class PhoneNumberProcessor(Transformer):
    """Process and validate Singapore phone numbers"""
    
    def __init__(self):
        super(PhoneNumberProcessor, self).__init__()
        self.singapore_country_codes = ['+65', '65', '0065']

    def _process_phone_udf(self):
        def process_phone(phone):
            if phone is None or not isinstance(phone, str):
                return None
            
            cleaned = re.sub(r'[\s\-\(\)]', '', str(phone))
            for country_code in self.singapore_country_codes:
                if cleaned.startswith(country_code):
                    cleaned = cleaned[len(country_code):]
                    break
            return cleaned
        return udf(process_phone, StringType())

    def _validate_phone_udf(self):
        def validate_phone(phone):
            if phone is None or not isinstance(phone, str):
                return False
            return bool(re.match(r'^[3689]\d{7}$', phone))
        return udf(validate_phone, BooleanType())

    def _transform(self, df):
        phone_columns = ['Primary_Phone_Number', 'Secondary_Mobile_Number', 'Landline_Phone_Number']
        result_df = df
        
        for col_name in phone_columns:
            if col_name in df.columns:
                result_df = (result_df
                    .withColumn(col_name, self._process_phone_udf()(col(col_name)))
                    .withColumn(f'{col_name}_Valid', self._validate_phone_udf()(col(col_name)))
                )
        return result_df

class EmailValidator(Transformer):
    """Validate email addresses and extract domain information"""
    
    def _validate_email_udf(self):
        def validate_email(email):
            if email is None or not isinstance(email, str):
                return False
            email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
            return bool(re.match(email_pattern, email))
        return udf(validate_email, BooleanType())

    def _extract_domain_udf(self):
        def extract_domain(email):
            if email is None or not isinstance(email, str) or '@' not in email:
                return None
            return email.split('@')[1].lower()
        return udf(extract_domain, StringType())

    def _is_legitimate_domain_udf(self):
        def is_legitimate_domain(domain):
            if domain is None:
                return False
            legitimate_domains = [
                'gmail.com', 'yahoo.com', 'hotmail.com', 'outlook.com', 'live.com',
                'icloud.com', 'protonmail.com', 'zoho.com', 'aol.com', 'mail.com',
                'gmail.com.sg', 'yahoo.com.sg', 'hotmail.com.sg', 'singnet.com.sg',
                'starhub.net.sg', 'pacific.net.sg'
            ]
            return domain in legitimate_domains
        return udf(is_legitimate_domain, BooleanType())

    def _is_disposable_domain_udf(self):
        def is_disposable_domain(domain):
            if domain is None:
                return False
            disposable_domains = [
                'tempmail.com', '10minutemail.com', 'guerrillamail.com',
                'mailinator.com', 'yopmail.com', 'trashmail.com',
                'disposableemail.com', 'fakeinbox.com', 'temp-mail.org'
            ]
            return domain in disposable_domains
        return udf(is_disposable_domain, BooleanType())

    def _transform(self, df):
        if 'Email_ID' in df.columns:
            return (df
                .withColumn('Email_Valid_Format', self._validate_email_udf()(col('Email_ID')))
                .withColumn('Email_Domain', self._extract_domain_udf()(col('Email_ID')))
                .withColumn('Email_Domain_Legitimate', self._is_legitimate_domain_udf()(col('Email_Domain')))
                .withColumn('Email_Disposable', self._is_disposable_domain_udf()(col('Email_Domain')))
            )
        return df

class PySparkDataPipeline:
    def __init__(self, file_path):
        self.file_path = file_path
        self.spark = SparkSession.builder \
            .appName("SingaporeLoanDataPipeline") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .getOrCreate()
        
        self.pipeline_stages = []
        self.processed_data = None
        self._build_pipeline()

    def _get_numeric_columns(self):
        return ['Age', 'Loan_Amount_SGD', 'Outstanding_Balance_SGD', 'Day_Past_Due',
                'Tenure', 'Interest_Rate', 'Current_EMI_SGD', 'Number_of_Past_Payments',
                'Amount_Paid_Each_Month_SGD', 'Bounce_History', 'Contact_History_Call_Attempts',
                'Contact_History_SMS', 'Contact_History_WhatsApp', 'Contact_History_EmailLogs',
                'No_of_Attempts', 'Average_Handling_Time', 'Credit_Score', 'Recent_Inquiries',
                'Loan_Exposure_Across_Banks', 'Recent_Score_Change', 'Unemployeement_rate_region',
                'Inflation_Rate', 'Interest_Rate_Trend', 'Economic_Stress_Index',
                'Income_Band_SGD', 'Digital_Engagement_Score']

    def _get_categorical_columns(self):
        return ['Product_Type', 'Payment_Frequency', 'Settlement_History',
                'Channel_used', 'Response_Outcome', 'Urban_Rural_Tag',
                'Language_Preference', 'Smartphone_Penetration', 'Preferred_Channel',
                'Call_SMS_Activity_Patterns', 'WhatsApp_OTT_usage_Indicator',
                'Regional_Time_Restrictions', 'Communication_Complaince_Limits',
                'Gender', 'Occupation', 'Employeement_Type']

    def _get_date_columns(self):
        return ['Installment_Due_Date', 'Last_Payment_Date']

    def _build_pipeline(self):
        """Build the PySpark pipeline"""
        self.pipeline_stages = [
            # Step 1: Process address information
            AddressProcessor(),
            
            # Step 2: Process phone numbers
            PhoneNumberProcessor(),
            
            # Step 3: Validate email addresses
            EmailValidator(),
            
            # Step 4: Convert boolean columns
            BooleanConverter(),
            
            # Step 5: Encode income bands
            IncomeBandEncoder(),
            
            # Step 6: Extract date features
            DateFeatureExtractor(self._get_date_columns()),
            
            # Step 7: Calculate digital engagement
            DigitalEngagementCalculator(),
        ]

    def _apply_standard_preprocessing(self, df):
        """Apply standard preprocessing for numeric and categorical columns"""
        
        # Get available columns
        numeric_features = [col for col in self._get_numeric_columns() if col in df.columns]
        categorical_features = [col for col in self._get_categorical_columns() if col in df.columns]
        
        stages = []
        
        # Handle numeric columns - imputation and scaling
        if numeric_features:
            # Impute numeric columns
            numeric_imputer = Imputer(
                inputCols=numeric_features,
                outputCols=[f"{col}_imputed" for col in numeric_features],
                strategy="median"
            )
            stages.append(numeric_imputer)
            
            # Scale numeric columns - create vectors first
            assemblers = []
            scalers = []
            
            for col_name in numeric_features:
                # Vector assembler for single column
                assembler = VectorAssembler(
                    inputCols=[f"{col_name}_imputed"],
                    outputCol=f"{col_name}_vec"
                )
                assemblers.append(assembler)
                
                # Standard scaler
                scaler = StandardScaler(
                    inputCol=f"{col_name}_vec",
                    outputCol=f"{col_name}_scaled",
                    withStd=True,
                    withMean=True
                )
                scalers.append(scaler)
            
            # Add all assemblers and scalers to stages
            stages.extend(assemblers)
            stages.extend(scalers)
        
        # Handle categorical columns - one-hot encoding
        indexers = []
        encoders = []
        
        for col_name in categorical_features:
            if col_name in df.columns:
                # String indexer
                indexer = StringIndexer(
                    inputCol=col_name,
                    outputCol=f"{col_name}_indexed",
                    handleInvalid="keep"
                )
                indexers.append(indexer)
                
                # One-hot encoder
                encoder = OneHotEncoder(
                    inputCols=[f"{col_name}_indexed"],
                    outputCols=[f"{col_name}_encoded"],
                    handleInvalid="keep"
                )
                encoders.append(encoder)
        
        stages.extend(indexers)
        stages.extend(encoders)
        
        return stages

    def _rename_columns_for_compatibility(self, df):
        """Rename columns to remove special characters for Spark compatibility"""
        # Replace spaces and special characters with underscores
        for old_col in df.columns:
            new_col = old_col.replace(' ', '_').replace('-', '_').replace('(', '').replace(')', '')
            if old_col != new_col:
                df = df.withColumnRenamed(old_col, new_col)
        return df

    def fit_transform(self, save_path=None):
        """Run the complete pipeline and return processed data"""
        print("=" * 50)
        print("RUNNING PYSPARK DATA PIPELINE")
        print("=" * 50)

        try:
            # Load data
            print("Loading data...")
            df = self.spark.read.csv(self.file_path, header=True, inferSchema=True)
            original_shape = (df.count(), len(df.columns))
            print(f"Successfully loaded {original_shape[0]:,} rows")

            # Rename columns for compatibility
            df = self._rename_columns_for_compatibility(df)

            # Apply custom transformations
            print("Applying preprocessing steps...")
            for stage in self.pipeline_stages:
                print(f"Applying {stage.__class__.__name__}...")
                df = stage.transform(df)

            # Remove duplicates
            df = df.dropDuplicates()
            print(f"After removing duplicates: {df.count():,} rows")

            # Apply standard preprocessing
            print("Applying standard preprocessing...")
            preprocessing_stages = self._apply_standard_preprocessing(df)
            
            if preprocessing_stages:
                pipeline = Pipeline(stages=preprocessing_stages)
                pipeline_model = pipeline.fit(df)
                df = pipeline_model.transform(df)

            # Create final column selection
            final_columns = []
            
            # Keep original identifier and non-processed columns
            base_columns = ['Customer_id', 'Loan_Account_id', 'Name']  # Add other base columns as needed
            
            for col in df.columns:
                # Skip intermediate processing columns
                if not any(x in col for x in ['_imputed', '_vec', '_indexed']):
                    final_columns.append(col)
            
            df = df.select(final_columns)
            
            # Convert vector columns to scalar columns for numeric features
            for col in self._get_numeric_columns():
                scaled_col = f"{col}_scaled"
                if scaled_col in df.columns:
                    # Extract the single value from the vector
                    df = df.withColumn(scaled_col, element_at(col(scaled_col), 1))

            self.processed_data = df

            # Save if path provided
            if save_path:
                self.save_processed_data(save_path)

            print("Pipeline completed successfully!")
            print(f"Original shape: {original_shape}")
            print(f"Processed shape: ({self.processed_data.count():,}, {len(self.processed_data.columns)})")

            return self.processed_data

        except Exception as e:
            print(f"Error in pipeline: {e}")
            import traceback
            traceback.print_exc()
            return None

    def save_processed_data(self, path):
        """Save the processed data"""
        if self.processed_data is not None:
            # Coalesce to a single partition for output
            (self.processed_data
             .coalesce(1)
             .write
             .mode("overwrite")
             .option("header", "true")
             .option("delimiter", ",")
             .csv(path))
            print(f"Processed data saved to {path}")

    def show_sample_results(self, num_rows=5):
        """Display sample results from the processed data"""
        if self.processed_data is not None:
            print("\nPipeline Summary:")
            print(f"Processed data shape: ({self.processed_data.count():,}, {len(self.processed_data.columns)})")
            
            print("\nSample of processed data:")
            self.processed_data.show(num_rows)
            
            print("\nData types after processing:")
            for col_name, col_type in self.processed_data.dtypes:
                print(f"{col_name}: {col_type}")
            
            # Show new columns created
            new_columns = ['City', 'Pincode', 'Email_Valid_Format', 'Email_Domain',
                          'Email_Domain_Legitimate', 'Email_Disposable']
            
            print("\nNew columns created by data cleaning:")
            for col in new_columns:
                if col in self.processed_data.columns:
                    non_null_count = self.processed_data.filter(col(col).isNotNull()).count()
                    print(f"{col}: {non_null_count:,} non-null values")
            
            # Show phone number validation results
            phone_columns = ['Primary_Phone_Number', 'Secondary_Mobile_Number', 'Landline_Phone_Number']
            for col in phone_columns:
                valid_col = f'{col}_Valid'
                if valid_col in self.processed_data.columns:
                    valid_count = self.processed_data.filter(col(valid_col) == True).count()
                    total_count = self.processed_data.count()
                    print(f"{valid_col}: {valid_count:,}/{total_count:,} valid numbers")

    def stop_spark(self):
        """Stop Spark session"""
        self.spark.stop()

# Example usage
if __name__ == "__main__":
    # Initialize the PySpark pipeline
    pyspark_pipeline = PySparkDataPipeline('singapore_loan_data.csv')
    
    try:
        # Run the complete pipeline
        processed_data = pyspark_pipeline.fit_transform(save_path='processed_data_pyspark')
        
        if processed_data is not None:
            # Display results
            pyspark_pipeline.show_sample_results()
        else:
            print("Pipeline failed to process data.")
            
    finally:
        # Stop Spark session
        pyspark_pipeline.stop_spark()
