from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, when, udf, lower, to_date, month, dayofweek, year
)
from pyspark.sql.types import BooleanType, StringType
import re

# -------------------------------------------
# Initialize Spark session
# -------------------------------------------
spark = SparkSession.builder.appName("SingaporeLoanDataPipeline").getOrCreate()

# -------------------------------------------
# Load CSV
# -------------------------------------------
def load_data(file_path):
    df = spark.read.option("header", True).option("inferSchema", True).csv(file_path)
    print(f"Loaded {df.count()} rows and {len(df.columns)} columns")
    return df

# -------------------------------------------
# Address Processor
# -------------------------------------------
def process_address(df):
    singapore_regions = [
        'Tengah', 'Loyang', 'Pasir Ris', 'Tampines', 'Clementi', 'Jurong',
        'Queenstown', 'Woodlands', 'Serangoon', 'Bedok', 'Ang Mo Kio',
        'Toa Payoh', 'Bishan', 'Bukit Merah', 'Bukit Timah', 'Geylang',
        'Kallang', 'Marine Parade', 'Novena', 'Potong Pasir', 'Punggol',
        'Sembawang', 'Sengkang', 'Hougang', 'Yishun', 'Lim Chu Kang',
        'Mandai', 'Sungei Kadut', 'Central Area', 'Downtown Core',
        'Newton', 'Orchard', 'River Valley', 'Rochor', 'Singapore River',
        'Southern Islands', 'Straits View', 'Outram', 'Museum',
        'Dhoby Ghaut', 'Marina South', 'Marina East', 'Marina Centre'
    ]

    @udf(StringType())
    def extract_city(address):
        if address is None:
            return None
        for region in singapore_regions:
            if region.lower() in address.lower():
                return region
        match = re.search(r'(\w+)\s+Singapore', address, re.IGNORECASE)
        return match.group(1) if match else "Unknown"

    @udf(StringType())
    def extract_pincode(address):
        if address is None:
            return None
        match = re.search(r'\b(\d{6})\b', address)
        return match.group(1) if match else None

    df = df.withColumn("City", extract_city(col("Address"))) \
           .withColumn("Pincode", extract_pincode(col("Address"))) \
           .drop("Address")
    return df

# -------------------------------------------
# Boolean Converter
# -------------------------------------------
def convert_booleans(df):
    bool_columns = [
        'Partial_Payment_Indicator', 'Repayment_Irregularity_Flags',
        'Mobile_Number_Active_Status', 'Email_Activity', 'Do_Not_Call_Registry_Data'
    ]
    for colname in bool_columns:
        if colname in df.columns:
            df = df.withColumn(
                colname,
                when(lower(col(colname)).isin(["true", "1", "yes"]), True).otherwise(False)
            )
    return df

# -------------------------------------------
# Income Band Encoder
# -------------------------------------------
def encode_income_band(df):
    mapping = {
        '50,000 or Below': 0,
        '50,000 to 100,000': 1,
        '100,000 to 200,000': 2,
        '200,000 to 300,000': 3,
        '300,000 to 500,000': 4,
        '500,000 or Above': 5
    }

    mapping_expr = when(col("Income_Band_SGD").isNull(), None)
    for k, v in mapping.items():
        mapping_expr = mapping_expr.when(col("Income_Band_SGD") == k, v)
    df = df.withColumn("Income_Band_SGD", mapping_expr.otherwise(None))
    return df

# -------------------------------------------
# Date Feature Extractor
# -------------------------------------------
def extract_date_features(df):
    date_columns = ['Installment_Due_Date', 'Last_Payment_Date']
    for date_col in date_columns:
        if date_col in df.columns:
            df = df.withColumn(date_col, to_date(col(date_col), "yyyy-MM-dd")) \
                   .withColumn(f"{date_col}_month", month(col(date_col))) \
                   .withColumn(f"{date_col}_dayofweek", dayofweek(col(date_col))) \
                   .withColumn(f"{date_col}_year", year(col(date_col))) \
                   .drop(date_col)
    return df

# -------------------------------------------
# Digital Engagement Calculator
# -------------------------------------------
def calculate_digital_engagement(df):
    required_cols = ['App_Login_Frequency', 'UPI_Transactions', 'Online_Banking_Activity']
    if all(c in df.columns for c in required_cols):
        df = df.withColumn(
            "Digital_Engagement_Score",
            (col("App_Login_Frequency") + col("UPI_Transactions") + col("Online_Banking_Activity")) / 3
        )
    return df

# -------------------------------------------
# Email Validator
# -------------------------------------------
def validate_email(df):
    legitimate_domains = [
        'gmail.com', 'yahoo.com', 'hotmail.com', 'outlook.com', 'live.com',
        'icloud.com', 'protonmail.com', 'zoho.com', 'aol.com', 'mail.com',
        'gmail.com.sg', 'yahoo.com.sg', 'hotmail.com.sg', 'singnet.com.sg',
        'starhub.net.sg', 'pacific.net.sg'
    ]
    disposable_domains = [
        'tempmail.com', '10minutemail.com', 'guerrillamail.com', 'mailinator.com',
        'yopmail.com', 'trashmail.com', 'disposableemail.com', 'fakeinbox.com', 'temp-mail.org'
    ]

    @udf(BooleanType())
    def email_valid(email):
        return bool(re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$', email or ''))

    @udf(StringType())
    def email_domain(email):
        if email and '@' in email:
            return email.split('@')[1].lower()
        return None

    @udf(BooleanType())
    def domain_legitimate(domain):
        return domain in legitimate_domains if domain else False

    @udf(BooleanType())
    def domain_disposable(domain):
        return domain in disposable_domains if domain else False

    df = df.withColumn("Email_Valid_Format", email_valid(col("Email_ID"))) \
           .withColumn("Email_Domain", email_domain(col("Email_ID"))) \
           .withColumn("Email_Domain_Legitimate", domain_legitimate(col("Email_Domain"))) \
           .withColumn("Email_Disposable", domain_disposable(col("Email_Domain")))
    return df

# -------------------------------------------
# Pipeline Runner
# -------------------------------------------
def run_pipeline(file_path, save_path=None):
    df = load_data(file_path)
    df = process_address(df)
    df = convert_booleans(df)
    df = encode_income_band(df)
    df = extract_date_features(df)
    df = calculate_digital_engagement(df)
    df = validate_email(df)
    df = df.dropDuplicates()

    print(f"Processed DataFrame shape: ({df.count()}, {len(df.columns)})")

    if save_path:
        df.write.mode("overwrite").option("header", True).csv(save_path)
        print(f"Processed data saved at: {save_path}")

    return df

# -------------------------------------------
# Run the PySpark pipeline
# -------------------------------------------
if _name_ == "_main_":
    processed_df = run_pipeline("singapore_loan_data.csv", save_path="processed_data_spark")
    processed_df.show(10, truncate=False)
