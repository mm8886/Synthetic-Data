from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.ml import Pipeline
from pyspark.ml.feature import StandardScaler, VectorAssembler, StringIndexer, OneHotEncoder
from pyspark.ml.base import Transformer
from pyspark.sql import DataFrame
import re
from itertools import chain

class DateFeatureExtractor(Transformer):
    """Extract features from date columns in PySpark"""
    
    def __init__(self, date_columns):
        super(DateFeatureExtractor, self).__init__()
        self.date_columns = date_columns
    
    def _transform(self, df):
        result_df = df
        
        for col_name in self.date_columns:
            if col_name in df.columns:
                # Convert to date first
                result_df = result_df.withColumn(
                    col_name, 
                    to_date(col(col_name), 'yyyy-MM-dd')
                )
                # Extract features
                result_df = (result_df
                    .withColumn(f'{col_name}_month', month(col(col_name)))
                    .withColumn(f'{col_name}_dayofweek', dayofweek(col(col_name)))
                    .withColumn(f'{col_name}_year', year(col(col_name)))
                    .drop(col_name)
                )
        
        return result_df

class IncomeBandEncoder(Transformer):
    """Encode income bands to numerical values in PySpark"""
    
    def __init__(self):
        super(IncomeBandEncoder, self).__init__()
        self.income_mapping = {
            '50,000 or Below': 0,
            '50,000 to 100,000': 1,
            '100,000 to 200,000': 2,
            '200,000 to 300,000': 3,
            '300,000 to 500,000': 4,
            '500,000 or Above': 5
        }
    
    def _transform(self, df):
        if 'Income_Band_SGD' not in df.columns:
            return df
        
        # FIXED: Create mapping expression without using sum()
        mapping_list = []
        for k, v in self.income_mapping.items():
            mapping_list.extend([lit(k), lit(v)])
        
        mapping_expr = create_map(mapping_list)
        
        return df.withColumn('Income_Band_SGD', 
                           mapping_expr[col('Income_Band_SGD')])

class BooleanConverter(Transformer):
    """Convert various boolean representations to proper booleans in PySpark"""
    
    def __init__(self):
        super(BooleanConverter, self).__init__()
    
    def _transform(self, df):
        bool_columns = ['Partial_Payment_Indicator', 'Repayment_Irregularity_Flags',
                       'Mobile_Number_Active_Status', 'Email_Activity', 'Do_Not_Call_Registry_Data']
        
        result_df = df
        
        for col_name in bool_columns:
            if col_name in df.columns:
                result_df = result_df.withColumn(
                    col_name,
                    when(lower(col(col_name)).isin(['true', '1', 'yes']), True)
                    .when(lower(col(col_name)).isin(['false', '0', 'no']), False)
                    .otherwise(None)
                )
        
        return result_df

class DigitalEngagementCalculator(Transformer):
    """Calculate digital engagement score in PySpark"""
    
    def __init__(self):
        super(DigitalEngagementCalculator, self).__init__()
    
    def _transform(self, df):
        engagement_metrics = ['App_Login_Frequency', 'UPI_Transactions', 'Online_Banking_Activity']
        
        if all(metric in df.columns for metric in engagement_metrics):
            return df.withColumn(
                'Digital_Engagement_Score',
                (col('App_Login_Frequency') + col('UPI_Transactions') + col('Online_Banking_Activity')) / 3
            )
        return df

class AddressProcessor(Transformer):
    """Extract city and pincode from address column in PySpark"""
    
    def __init__(self):
        super(AddressProcessor, self).__init__()
        self.singapore_regions = [
            'Tengah', 'Loyang', 'Pasir Ris', 'Tampines', 'Clementi', 'Jurong',
            'Queenstown', 'Woodlands', 'Serangoon', 'Bedok', 'Ang Mo Kio',
            'Toa Payoh', 'Bishan', 'Bukit Merah', 'Bukit Timah', 'Geylang',
            'Kallang', 'Marine Parade', 'Novena', 'Potong Pasir', 'Punggol',
            'Sembawang', 'Sengkang', 'Hougang', 'Yishun', 'Lim Chu Kang',
            'Mandai', 'Sungei Kadut', 'Central Area', 'Downtown Core',
            'Newton', 'Orchard', 'River Valley', 'Rochor', 'Singapore River',
            'Southern Islands', 'Straits View', 'Outram', 'Museum',
            'Dhoby Ghaut', 'Marina South', 'Marina East', 'Marina Centre'
        ]
    
    def _extract_city_udf(self):
        """UDF for extracting city from address"""
        def extract_city(address):
            if address is None:
                return "Unknown"
            
            address_str = str(address)
            address_lower = address_str.lower()
            
            for region in self.singapore_regions:
                if region.lower() in address_lower:
                    return region
            
            # Fallback: extract the word before "Singapore"
            match = re.search(r'(\w+)\s+Singapore', address_str, re.IGNORECASE)
            if match:
                return match.group(1)
            
            return "Unknown"
        
        return udf(extract_city, StringType())
    
    def _extract_pincode_udf(self):
        """UDF for extracting pincode from address"""
        def extract_pincode(address):
            if address is None:
                return None
            
            pincode_pattern = r'\b(\d{6})\b'
            match = re.search(pincode_pattern, str(address))
            if match:
                return match.group(1)
            return None
        
        return udf(extract_pincode, StringType())
    
    def _transform(self, df):
        if 'Address' not in df.columns:
            return df
        
        extract_city_udf = self._extract_city_udf()
        extract_pincode_udf = self._extract_pincode_udf()
        
        return (df
            .withColumn('City', extract_city_udf(col('Address')))
            .withColumn('Pincode', extract_pincode_udf(col('Address')))
            .drop('Address')
        )

class PhoneNumberProcessor(Transformer):
    """Process and validate Singapore phone numbers in PySpark"""
    
    def __init__(self):
        super(PhoneNumberProcessor, self).__init__()
        self.singapore_country_codes = ['+65', '65', '0065']
    
    def _process_phone_udf(self):
        """UDF to process phone number"""
        def process_phone(phone):
            if phone is None:
                return None
            
            phone_str = str(phone)
            cleaned = re.sub(r'[\s\-\(\)]', '', phone_str)
            
            for country_code in self.singapore_country_codes:
                if cleaned.startswith(country_code):
                    cleaned = cleaned[len(country_code):]
                    break
            
            return cleaned
        
        return udf(process_phone, StringType())
    
    def _validate_singapore_udf(self):
        """UDF to validate Singapore phone number"""
        def validate_phone(phone):
            if phone is None:
                return False
            
            phone_str = str(phone)
            return bool(re.match(r'^[3689]\d{7}$', phone_str))
        
        return udf(validate_phone, BooleanType())
    
    def _transform(self, df):
        phone_columns = ['Primary_Phone_Number', 'Secondary_Mobile_Number', 'Landline_Phone_Number']
        
        result_df = df
        process_udf = self._process_phone_udf()
        validate_udf = self._validate_singapore_udf()
        
        for col_name in phone_columns:
            if col_name in df.columns:
                # Process phone number
                result_df = result_df.withColumn(
                    col_name,
                    process_udf(col(col_name))
                )
                # Add validation flag
                result_df = result_df.withColumn(
                    f'{col_name}_Valid',
                    validate_udf(col(col_name))
                )
        
        return result_df

class EmailValidator(Transformer):
    """Validate email addresses and extract domain information in PySpark"""
    
    def __init__(self):
        super(EmailValidator, self).__init__()
    
    def _validate_email_udf(self):
        """UDF for email validation"""
        def validate_email(email):
            if email is None:
                return False
            
            email_str = str(email)
            email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
            return bool(re.match(email_pattern, email_str))
        
        return udf(validate_email, BooleanType())
    
    def _extract_domain_udf(self):
        """UDF for domain extraction"""
        def extract_domain(email):
            if email is None:
                return None
            
            email_str = str(email)
            if '@' not in email_str:
                return None
            
            return email_str.split('@')[1].lower()
        
        return udf(extract_domain, StringType())
    
    def _is_legitimate_domain_udf(self):
        """UDF for legitimate domain check"""
        def is_legitimate_domain(domain):
            if domain is None:
                return False
            
            legitimate_domains = [
                'gmail.com', 'yahoo.com', 'hotmail.com', 'outlook.com', 'live.com',
                'icloud.com', 'protonmail.com', 'zoho.com', 'aol.com', 'mail.com',
                'gmail.com.sg', 'yahoo.com.sg', 'hotmail.com.sg', 'singnet.com.sg',
                'starhub.net.sg', 'pacific.net.sg'
            ]
            return domain in legitimate_domains
        
        return udf(is_legitimate_domain, BooleanType())
    
    def _is_disposable_domain_udf(self):
        """UDF for disposable domain check"""
        def is_disposable_domain(domain):
            if domain is None:
                return False
            
            disposable_domains = [
                'tempmail.com', '10minutemail.com', 'guerrillamail.com',
                'mailinator.com', 'yopmail.com', 'trashmail.com',
                'disposableemail.com', 'fakeinbox.com', 'temp-mail.org'
            ]
            return domain in disposable_domains
        
        return udf(is_disposable_domain, BooleanType())
    
    def _transform(self, df):
        if 'Email_ID' not in df.columns:
            return df
        
        validate_email_udf = self._validate_email_udf()
        extract_domain_udf = self._extract_domain_udf()
        is_legitimate_domain_udf = self._is_legitimate_domain_udf()
        is_disposable_domain_udf = self._is_disposable_domain_udf()
        
        result_df = (df
            .withColumn('Email_Valid_Format', validate_email_udf(col('Email_ID')))
            .withColumn('Email_Domain', extract_domain_udf(col('Email_ID')))
        )
        
        result_df = (result_df
            .withColumn('Email_Domain_Legitimate', is_legitimate_domain_udf(col('Email_Domain')))
            .withColumn('Email_Disposable', is_disposable_domain_udf(col('Email_Domain')))
        )
        
        return result_df

class PysparkDataPipeline:
    def __init__(self, file_path, spark_session=None):
        self.file_path = file_path
        self.spark = spark_session or SparkSession.builder \
            .appName("LoanDataPipeline") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .getOrCreate()
        
        # Set log level to WARN to reduce verbose output
        self.spark.sparkContext.setLogLevel("WARN")
        
        self.pipeline_stages = None
        self.processed_data = None
        self._build_pipeline()
    
    def _get_numeric_columns(self):
        """Define numeric columns for preprocessing"""
        return ['Age', 'Loan_Amount_SGD', 'Outstanding_Balance_SGD', 'Day_Past_Due',
                'Tenure', 'Interest_Rate', 'Current_EMI_SGD', 'Number_of_Past_Payments',
                'Amount_Paid_Each_Month_SGD', 'Bounce_History', 'Contact_History_Call_Attempts',
                'Contact_History_SMS', 'Contact_History_WhatsApp', 'Contact_History_EmailLogs',
                'No_of_Attempts', 'Average_Handling_Time', 'Credit_Score', 'Recent_Inquiries',
                'Loan_Exposure_Across_Banks', 'Recent_Score_Change', 'Unemployeement_rate_region',
                'Inflation_Rate', 'Interest_Rate_Trend', 'Economic_Stress_Index',
                'Income_Band_SGD', 'Digital_Engagement_Score']
    
    def _get_categorical_columns(self):
        """Define categorical columns for preprocessing"""
        return ['Product_Type', 'Payment_Frequency', 'Settlement_History',
                'Channel_used', 'Response_Outcome', 'Urban_Rural_Tag',
                'Language_Preference', 'Smartphone_Penetration', 'Preferred_Channel',
                'Call_SMS_Activity_Patterns', 'WhatsApp_OTT_usage_Indicator',
                'Regional_Time_Restrictions', 'Communication_Complaince_Limits',
                'Gender', 'Occupation', 'Employeement_Type']
    
    def _get_date_columns(self):
        """Define date columns for processing"""
        return ['Installment_Due_Date', 'Last_Payment_Date']
    
    def _build_pipeline(self):
        """Build the PySpark pipeline"""
        self.pipeline_stages = [
            # Step 1: Process address information
            AddressProcessor(),
            
            # Step 2: Process phone numbers
            PhoneNumberProcessor(),
            
            # Step 3: Validate email addresses
            EmailValidator(),
            
            # Step 4: Convert boolean columns
            BooleanConverter(),
            
            # Step 5: Encode income bands - FIXED VERSION
            IncomeBandEncoder(),
            
            # Step 6: Extract date features
            DateFeatureExtractor(self._get_date_columns()),
            
            # Step 7: Calculate digital engagement
            DigitalEngagementCalculator(),
        ]
    
    def _handle_missing_values(self, df):
        """Handle missing values in the DataFrame"""
        print("Handling missing values...")
        
        # Numeric columns: fill with median or 0
        numeric_columns = [col for col in self._get_numeric_columns() if col in df.columns]
        for col_name in numeric_columns:
            # Calculate median or use 0 if calculation fails
            try:
                median_val = df.select(mean(col(col_name))).collect()[0][0]
                if median_val is None:
                    median_val = 0
                df = df.fillna({col_name: median_val})
            except:
                df = df.fillna({col_name: 0})
        
        # Categorical columns: fill with 'Unknown'
        categorical_columns = [col for col in self._get_categorical_columns() if col in df.columns]
        for col_name in categorical_columns:
            df = df.fillna({col_name: 'Unknown'})
        
        return df
    
    def fit_transform(self, save_path=None):
        """Run the complete pipeline and return processed data"""
        print("=" * 50)
        print("RUNNING PYSPARK DATA PIPELINE")
        print("=" * 50)
        
        try:
            # Load data
            print("Loading data...")
            df = self.spark.read.csv(self.file_path, header=True, inferSchema=True)
            original_count = df.count()
            original_columns = len(df.columns)
            print(f"Successfully loaded {original_count:,} rows with {original_columns} columns")
            
            # Apply custom transformations
            print("Applying preprocessing steps...")
            for i, stage in enumerate(self.pipeline_stages, 1):
                print(f"Step {i}/{len(self.pipeline_stages)}: Applying {stage.__class__.__name__}...")
                df = stage.transform(df)
                print(f"  Shape after {stage.__class__.__name__}: ({df.count()}, {len(df.columns)})")
            
            # Remove duplicates
            df = df.dropDuplicates()
            print(f"After removing duplicates: {df.count()} rows")
            
            # Handle missing values
            df = self._handle_missing_values(df)
            
            self.processed_data = df
            
            # Save if path provided
            if save_path:
                self.save_processed_data(save_path)
            
            print("Pipeline completed successfully!")
            print(f"Original shape: ({original_count}, {original_columns})")
            print(f"Processed shape: ({self.processed_data.count()}, {len(self.processed_data.columns)})")
            
            return self.processed_data
            
        except Exception as e:
            print(f"Error in pipeline: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def save_processed_data(self, path):
        """Save the processed data"""
        if self.processed_data is not None:
            # Use coalesce to reduce number of partitions for better write performance
            (self.processed_data.coalesce(1)
             .write
             .mode('overwrite')
             .option("header", "true")
             .csv(path + "_csv"))
            
            print(f"Processed data saved to {path}_csv")
    
    def load_and_transform_new_data(self, new_data_path):
        """Load and transform new data"""
        new_df = self.spark.read.csv(new_data_path, header=True, inferSchema=True)
        
        # Apply the same transformations
        for stage in self.pipeline_stages:
            new_df = stage.transform(new_df)
        
        return new_df

# Example usage with error handling
if __name__ == "__main__":
    try:
        # Initialize the PySpark pipeline
        print("Initializing PySpark Pipeline...")
        pyspark_pipeline = PysparkDataPipeline('singapore_loan_data.csv')
        
        # Run the complete pipeline
        processed_data = pyspark_pipeline.fit_transform(save_path='processed_data')
        
        if processed_data is not None:
            # Display results
            print("\n" + "="*50)
            print("PIPELINE EXECUTION SUMMARY")
            print("="*50)
            print(f"Final processed data shape: ({processed_data.count()}, {len(processed_data.columns)})")
            
            print("\nFirst 5 rows of processed data:")
            processed_data.show(5, truncate=False)
            
            print("\nData schema:")
            processed_data.printSchema()
            
            # Show the new columns created by the data cleaning processes
            new_columns = ['City', 'Pincode', 'Email_Valid_Format', 'Email_Domain',
                          'Email_Domain_Legitimate', 'Email_Disposable']
            
            print("\nNew columns created by data cleaning:")
            for col in new_columns:
                if col in processed_data.columns:
                    non_null_count = processed_data.filter(col(col).isNotNull()).count()
                    total_count = processed_data.count()
                    print(f"  {col}: {non_null_count}/{total_count} non-null values")
            
            # Show phone number validation results
            phone_columns = ['Primary_Phone_Number', 'Secondary_Mobile_Number', 'Landline_Phone_Number']
            print("\nPhone number validation results:")
            for col in phone_columns:
                valid_col = f'{col}_Valid'
                if valid_col in processed_data.columns:
                    valid_count = processed_data.filter(col(valid_col) == True).count()
                    total_count = processed_data.count()
                    print(f"  {valid_col}: {valid_count}/{total_count} valid numbers")
        
        else:
            print("Pipeline failed to process data.")
    
    except Exception as e:
        print(f"Fatal error in main execution: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # Stop Spark session
        if 'pyspark_pipeline' in locals() and hasattr(pyspark_pipeline, 'spark'):
            pyspark_pipeline.spark.stop()
            print("\nSpark session stopped.")