from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.ml import Pipeline
from pyspark.ml.feature import StandardScaler, OneHotEncoder, StringIndexer, VectorAssembler, Imputer
from pyspark.ml.base import Transformer, Estimator
from pyspark.sql import DataFrame
import re
import warnings
from datetime import datetime
from itertools import chain

warnings.filterwarnings('ignore')

# Initialize Spark Session
spark = SparkSession.builder \
    .appName("SingaporeLoanDataPipeline") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .getOrCreate()

# Custom PySpark Transformers
class DateFeatureExtractor(Transformer):
    """Extract features from date columns"""
    def __init__(self, date_columns):
        super().__init__()
        self.date_columns = date_columns

    def _transform(self, df):
        result_df = df
        for col in self.date_columns:
            if col in df.columns:
                result_df = result_df \
                    .withColumn(col, to_date(col(col), "yyyy-MM-dd")) \
                    .withColumn(f'{col}_month', month(col(col))) \
                    .withColumn(f'{col}_dayofweek', dayofweek(col(col))) \
                    .withColumn(f'{col}_year', year(col(col))) \
                    .drop(col)
        return result_df

class IncomeBandEncoder(Transformer):
    """Encode income bands to numerical values"""
    def __init__(self):
        super().__init__()
        self.income_mapping = {
            '50,000 or Below': 0,
            '50,000 to 100,000': 1,
            '100,000 to 200,000': 2,
            '200,000 to 300,000': 3,
            '300,000 to 500,000': 4,
            '500,000 or Above': 5
        }

    def _transform(self, df):
        if 'Income_Band_SGD' in df.columns:
            # Create mapping expression using chain to flatten the dictionary items
            mapping_list = []
            for key, value in self.income_mapping.items():
                mapping_list.extend([lit(key), lit(value)])
            
            mapping_expr = create_map(mapping_list)
            return df.withColumn('Income_Band_SGD', mapping_expr[col('Income_Band_SGD')])
        return df

class BooleanConverter(Transformer):
    """Convert various boolean representations to proper booleans"""
    def __init__(self):
        super().__init__()

    def _transform(self, df):
        bool_columns = ['Partial_Payment_Indicator', 'Repayment_Irregularity_Flags',
                       'Mobile_Number_Active_Status', 'Email_Activity', 'Do_Not_Call_Registry_Data']
        
        result_df = df
        for col_name in bool_columns:
            if col_name in df.columns:
                result_df = result_df.withColumn(
                    col_name,
                    when(lower(col(col_name)).isin(['true', '1', 'yes']), True)
                    .when(lower(col(col_name)).isin(['false', '0', 'no']), False)
                    .otherwise(None)
                )
        return result_df

class DigitalEngagementCalculator(Transformer):
    """Calculate digital engagement score"""
    def __init__(self):
        super().__init__()

    def _transform(self, df):
        engagement_metrics = ['App_Login_Frequency', 'UPI_Transactions', 'Online_Banking_Activity']
        if all(metric in df.columns for metric in engagement_metrics):
            return df.withColumn(
                'Digital_Engagement_Score',
                (col('App_Login_Frequency') + col('UPI_Transactions') + col('Online_Banking_Activity')) / 3
            )
        return df

class AddressProcessor(Transformer):
    """Extract city and pincode from address column"""
    def __init__(self):
        super().__init__()
        self.singapore_regions = [
            'Tengah', 'Loyang', 'Pasir Ris', 'Tampines', 'Clementi', 'Jurong',
            'Queenstown', 'Woodlands', 'Serangoon', 'Bedok', 'Ang Mo Kio',
            'Toa Payoh', 'Bishan', 'Bukit Merah', 'Bukit Timah', 'Geylang',
            'Kallang', 'Marine Parade', 'Novena', 'Potong Pasir', 'Punggol',
            'Sembawang', 'Sengkang', 'Hougang', 'Yishun', 'Lim Chu Kang',
            'Mandai', 'Sungei Kadut', 'Central Area', 'Downtown Core',
            'Newton', 'Orchard', 'River Valley', 'Rochor', 'Singapore River',
            'Southern Islands', 'Straits View', 'Outram', 'Museum',
            'Dhoby Ghaut', 'Marina South', 'Marina East', 'Marina Centre'
        ]

    def _extract_city_udf(self):
        def extract_city(address):
            if address is None:
                return None
            address_lower = address.lower()
            for region in self.singapore_regions:
                if region.lower() in address_lower:
                    return region
            match = re.search(r'(\w+)\s+Singapore', address, re.IGNORECASE)
            if match:
                return match.group(1)
            return 'Unknown'
        return udf(extract_city, StringType())

    def _extract_pincode_udf(self):
        def extract_pincode(address):
            if address is None:
                return None
            pincode_pattern = r'\b(\d{6})\b'
            match = re.search(pincode_pattern, address)
            if match:
                return match.group(1)
            return None
        return udf(extract_pincode, StringType())

    def _transform(self, df):
        if 'Address' in df.columns:
            return df \
                .withColumn('City', self._extract_city_udf()(col('Address'))) \
                .withColumn('Pincode', self._extract_pincode_udf()(col('Address'))) \
                .drop('Address')
        return df

class PhoneNumberProcessor(Transformer):
    """Process and validate Singapore phone numbers"""
    def __init__(self):
        super().__init__()
        self.singapore_country_codes = ['+65', '65', '0065']

    def _process_phone_udf(self):
        def process_phone(phone):
            if phone is None or not isinstance(phone, str):
                return None
            cleaned = re.sub(r'[\s\-\(\)]', '', str(phone))
            for country_code in self.singapore_country_codes:
                if cleaned.startswith(country_code):
                    cleaned = cleaned[len(country_code):]
                    break
            return cleaned
        return udf(process_phone, StringType())

    def _validate_phone_udf(self):
        def validate_phone(phone):
            if phone is None or not isinstance(phone, str):
                return False
            return bool(re.match(r'^[3689]\d{7}$', phone))
        return udf(validate_phone, BooleanType())

    def _transform(self, df):
        phone_columns = ['Primary_Phone_Number', 'Secondary_Mobile_Number', 'Landline_Phone_Number']
        result_df = df
        for col_name in phone_columns:
            if col_name in df.columns:
                result_df = result_df \
                    .withColumn(col_name, self._process_phone_udf()(col(col_name))) \
                    .withColumn(f'{col_name}_Valid', self._validate_phone_udf()(col(col_name)))
        return result_df

class EmailValidator(Transformer):
    """Validate email addresses and extract domain information"""
    def __init__(self):
        super().__init__()
        self.legitimate_domains = [
            'gmail.com', 'yahoo.com', 'hotmail.com', 'outlook.com', 'live.com',
            'icloud.com', 'protonmail.com', 'zoho.com', 'aol.com', 'mail.com',
            'gmail.com.sg', 'yahoo.com.sg', 'hotmail.com.sg', 'singnet.com.sg',
            'starhub.net.sg', 'pacific.net.sg'
        ]
        self.disposable_domains = [
            'tempmail.com', '10minutemail.com', 'guerrillamail.com',
            'mailinator.com', 'yopmail.com', 'trashmail.com',
            'disposableemail.com', 'fakeinbox.com', 'temp-mail.org'
        ]

    def _validate_email_udf(self):
        def validate_email(email):
            if email is None or not isinstance(email, str):
                return False
            email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
            return bool(re.match(email_pattern, email))
        return udf(validate_email, BooleanType())

    def _extract_domain_udf(self):
        def extract_domain(email):
            if email is None or not isinstance(email, str) or '@' not in email:
                return None
            return email.split('@')[1].lower()
        return udf(extract_domain, StringType())

    def _is_legitimate_domain_udf(self):
        def is_legitimate_domain(domain):
            if domain is None:
                return False
            return domain in self.legitimate_domains
        return udf(is_legitimate_domain, BooleanType())

    def _is_disposable_domain_udf(self):
        def is_disposable_domain(domain):
            if domain is None:
                return False
            return domain in self.disposable_domains
        return udf(is_disposable_domain, BooleanType())

    def _transform(self, df):
        if 'Email_ID' in df.columns:
            return df \
                .withColumn('Email_Valid_Format', self._validate_email_udf()(col('Email_ID'))) \
                .withColumn('Email_Domain', self._extract_domain_udf()(col('Email_ID'))) \
                .withColumn('Email_Domain_Legitimate', self._is_legitimate_domain_udf()(col('Email_Domain'))) \
                .withColumn('Email_Disposable', self._is_disposable_domain_udf()(col('Email_Domain')))
        return df

class DataLoader(Transformer):
    """Load and initial data processing"""
    def __init__(self, file_path):
        super().__init__()
        self.file_path = file_path

    def _transform(self, df):
        print("Loading data...")
        try:
            # Read the CSV file
            df_loaded = spark.read.csv(self.file_path, header=True, inferSchema=True)
            print(f"Successfully loaded {df_loaded.count():,} rows")
            return df_loaded
        except Exception as e:
            print(f"Error loading data: {e}")
            # Return empty DataFrame with expected structure
            return spark.createDataFrame([], StructType([]))

class PySparkDataPipeline:
    def __init__(self, file_path):
        self.file_path = file_path
        self.pipeline = None
        self.processed_data = None
        self.numeric_columns = []
        self.categorical_columns = []
        self._build_pipeline()

    def _get_numeric_columns(self):
        """Define numeric columns for preprocessing"""
        return ['Age', 'Loan_Amount_SGD', 'Outstanding_Balance_SGD', 'Day_Past_Due',
                'Tenure', 'Interest_Rate', 'Current_EMI_SGD', 'Number_of_Past_Payments',
                'Amount_Paid_Each_Month_SGD', 'Bounce_History', 'Contact_History_Call_Attempts',
                'Contact_History_SMS', 'Contact_History_WhatsApp', 'Contact_History_EmailLogs',
                'No_of_Attempts', 'Average_Handling_Time', 'Credit_Score', 'Recent_Inquiries',
                'Loan_Exposure_Across_Banks', 'Recent_Score_Change', 'Unemployeement_rate_region',
                'Inflation_Rate', 'Interest_Rate_Trend', 'Economic_Stress_Index',
                'Income_Band_SGD', 'Digital_Engagement_Score']

    def _get_categorical_columns(self):
        """Define categorical columns for preprocessing"""
        return ['Product_Type', 'Payment_Frequency', 'Settlement_History',
                'Channel_used', 'Response_Outcome', 'Urban_Rural_Tag',
                'Language_Preference', 'Smartphone_Penetration', 'Preferred_Channel',
                'Call_SMS_Activity_Patterns', 'WhatsApp_OTT_usage_Indicator',
                'Regional_Time_Restrictions', 'Communication_Complaince_Limits',
                'Gender', 'Occupation', 'Employeement_Type']

    def _get_date_columns(self):
        """Define date columns for processing"""
        return ['Installment_Due_Date', 'Last_Payment_Date']

    def _build_pipeline(self):
        """Build the PySpark pipeline"""
        self.pipeline = Pipeline(stages=[
            DataLoader(self.file_path),
            AddressProcessor(),
            PhoneNumberProcessor(),
            EmailValidator(),
            BooleanConverter(),
            IncomeBandEncoder(),
            DateFeatureExtractor(self._get_date_columns()),
            DigitalEngagementCalculator()
        ])

    def fit_transform(self, save_path=None):
        """Run the complete pipeline and return processed data"""
        print("=" * 50)
        print("RUNNING PYSPARK DATA PIPELINE")
        print("=" * 50)

        try:
            # Fit the pipeline on empty DataFrame to initialize
            pipeline_model = self.pipeline.fit(spark.createDataFrame([], StructType([])))
            
            # Transform the actual data
            self.processed_data = pipeline_model.transform(spark.createDataFrame([], StructType([])))
            
            # Cache the processed data for faster access
            self.processed_data.cache()
            
            original_count = self.processed_data.count()
            print(f"Data loaded and transformed: {original_count:,} rows")
            
            # Apply additional preprocessing
            self._apply_standard_preprocessing()

            # Remove duplicates
            self.processed_data = self.processed_data.dropDuplicates()
            final_count = self.processed_data.count()
            print(f"After removing duplicates: {final_count:,} rows")

            # Save if path provided
            if save_path:
                self.save_processed_data(save_path)

            print("Pipeline completed successfully!")
            print(f"Final processed shape: ({self.processed_data.count():,}, {len(self.processed_data.columns)})")

            return self.processed_data

        except Exception as e:
            print(f"Error in pipeline: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _apply_standard_preprocessing(self):
        """Apply standard preprocessing to numeric and categorical columns"""
        # Get current columns after transformations
        current_columns = self.processed_data.columns
        
        # Filter numeric and categorical columns that actually exist in the DataFrame
        self.numeric_columns = [col for col in self._get_numeric_columns() if col in current_columns]
        self.categorical_columns = [col for col in self._get_categorical_columns() if col in current_columns]
        
        print(f"Processing {len(self.numeric_columns)} numeric columns")
        print(f"Processing {len(self.categorical_columns)} categorical columns")

        # Handle numeric columns
        if self.numeric_columns:
            print("Applying numeric preprocessing...")
            
            # First, handle null values in numeric columns
            for col_name in self.numeric_columns:
                self.processed_data = self.processed_data.fillna(0, subset=[col_name])

            # Use VectorAssembler for numeric columns
            assembler = VectorAssembler(
                inputCols=self.numeric_columns,
                outputCol="numeric_features"
            )
            
            # Scale numeric columns
            scaler = StandardScaler(
                inputCol="numeric_features",
                outputCol="scaled_numeric_features",
                withStd=True,
                withMean=True
            )
            
            # Apply numeric preprocessing
            self.processed_data = assembler.transform(self.processed_data)
            scaler_model = scaler.fit(self.processed_data)
            self.processed_data = scaler_model.transform(self.processed_data)
            
            # Drop original numeric columns and intermediate features
            self.processed_data = self.processed_data.drop(*self.numeric_columns)
            self.processed_data = self.processed_data.drop("numeric_features")

        # Handle categorical columns
        if self.categorical_columns:
            print("Applying categorical preprocessing...")
            
            # Handle null values in categorical columns
            for col_name in self.categorical_columns:
                self.processed_data = self.processed_data.fillna("Unknown", subset=[col_name])

            # String index categorical columns
            indexers = []
            indexed_columns = []
            
            for col_name in self.categorical_columns:
                indexed_col = f"{col_name}_indexed"
                indexer = StringIndexer(
                    inputCol=col_name, 
                    outputCol=indexed_col, 
                    handleInvalid="keep"
                )
                indexers.append(indexer)
                indexed_columns.append(indexed_col)

            # Fit and transform indexers
            for indexer in indexers:
                indexer_model = indexer.fit(self.processed_data)
                self.processed_data = indexer_model.transform(self.processed_data)

            # One-hot encode indexed columns
            encoder = OneHotEncoder(
                inputCols=indexed_columns,
                outputCols=[f"{col}_encoded" for col in self.categorical_columns]
            )
            
            encoder_model = encoder.fit(self.processed_data)
            self.processed_data = encoder_model.transform(self.processed_data)
            
            # Drop original categorical and intermediate indexed columns
            columns_to_drop = self.categorical_columns + indexed_columns
            self.processed_data = self.processed_data.drop(*columns_to_drop)

    def save_processed_data(self, path):
        """Save the processed data"""
        if self.processed_data is not None:
            # Use coalesce to reduce the number of partitions for writing
            self.processed_data.coalesce(1).write.mode("overwrite").parquet(path)
            print(f"Processed data saved to {path}")

    def load_and_transform_new_data(self, new_data_path):
        """Load and transform new data using the fitted pipeline"""
        try:
            # Load new data
            new_df = spark.read.csv(new_data_path, header=True, inferSchema=True)
            
            # Fit pipeline on empty DataFrame and transform new data
            pipeline_model = self.pipeline.fit(spark.createDataFrame([], StructType([])))
            transformed_df = pipeline_model.transform(new_df)
            
            return transformed_df
        except Exception as e:
            print(f"Error transforming new data: {e}")
            return None

# Example usage
if __name__ == "__main__":
    # Initialize the PySpark pipeline
    pyspark_pipeline = PySparkDataPipeline('singapore_loan_data.csv')

    # Run the complete pipeline
    processed_data = pyspark_pipeline.fit_transform(save_path='processed_data_parquet')

    if processed_data is not None:
        # Display results
        print("\nPipeline Summary:")
        print(f"Processed data shape: ({processed_data.count():,}, {len(processed_data.columns)})")

        print("\nSample of processed data:")
        processed_data.show(5, truncate=False)

        print("\nData schema after processing:")
        processed_data.printSchema()

        # Show the new columns created by the data cleaning processes
        new_columns = ['City', 'Pincode', 'Email_Valid_Format', 'Email_Domain',
                      'Email_Domain_Legitimate', 'Email_Disposable']

        print("\nNew columns created by data cleaning:")
        for col in new_columns:
            if col in processed_data.columns:
                non_null_count = processed_data.filter(col(col).isNotNull()).count()
                print(f"{col}: {non_null_count:,} non-null values")

        # Show phone number validation results
        phone_columns = ['Primary_Phone_Number', 'Secondary_Mobile_Number', 'Landline_Phone_Number']
        for col in phone_columns:
            valid_col = f'{col}_Valid'
            if valid_col in processed_data.columns:
                valid_count = processed_data.filter(col(valid_col) == True).count()
                total_count = processed_data.count()
                print(f"{valid_col}: {valid_count:,}/{total_count:,} valid numbers")
                
        # Show memory usage
        print(f"\nNumber of partitions: {processed_data.rdd.getNumPartitions()}")
        
    else:
        print("Pipeline failed to process data.")

    # Stop Spark session
    spark.stop()
