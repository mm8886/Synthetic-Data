from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.ml import Pipeline
from pyspark.ml.feature import StandardScaler, OneHotEncoder, StringIndexer, VectorAssembler, Imputer
from pyspark.ml.base import Transformer, Estimator
from pyspark.sql import DataFrame
import re
import warnings
from datetime import datetime

warnings.filterwarnings('ignore')

# Initialize Spark Session
spark = SparkSession.builder \
    .appName("SingaporeLoanDataPipeline") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .getOrCreate()

# Custom PySpark Transformers
class DateFeatureExtractor(Transformer):
    """Extract features from date columns"""
    def __init__(self, date_columns):
        super().__init__()
        self.date_columns = date_columns

    def _transform(self, df):
        result_df = df
        for col in self.date_columns:
            if col in df.columns:
                result_df = result_df \
                    .withColumn(col, to_date(col(col), "yyyy-MM-dd")) \
                    .withColumn(f'{col}_month', month(col(col))) \
                    .withColumn(f'{col}_dayofweek', dayofweek(col(col))) \
                    .withColumn(f'{col}_year', year(col(col))) \
                    .drop(col)
        return result_df

class IncomeBandEncoder(Transformer):
    """Encode income bands to numerical values"""
    def __init__(self):
        super().__init__()
        self.income_mapping = {
            '50,000 or Below': 0,
            '50,000 to 100,000': 1,
            '100,000 to 200,000': 2,
            '200,000 to 300,000': 3,
            '300,000 to 500,000': 4,
            '500,000 or Above': 5
        }

    def _transform(self, df):
        if 'Income_Band_SGD' in df.columns:
            mapping_expr = create_map([lit(x) for x in sum(self.income_mapping.items(), ())])
            return df.withColumn('Income_Band_SGD', mapping_expr[col('Income_Band_SGD')])
        return df

class BooleanConverter(Transformer):
    """Convert various boolean representations to proper booleans"""
    def __init__(self):
        super().__init__()

    def _transform(self, df):
        bool_columns = ['Partial_Payment_Indicator', 'Repayment_Irregularity_Flags',
                       'Mobile_Number_Active_Status', 'Email_Activity', 'Do_Not_Call_Registry_Data']
        
        result_df = df
        for col_name in bool_columns:
            if col_name in df.columns:
                result_df = result_df.withColumn(
                    col_name,
                    when(lower(col(col_name)).isin(['true', '1', 'yes']), True)
                    .when(lower(col(col_name)).isin(['false', '0', 'no']), False)
                    .otherwise(None)
                )
        return result_df

class DigitalEngagementCalculator(Transformer):
    """Calculate digital engagement score"""
    def __init__(self):
        super().__init__()

    def _transform(self, df):
        engagement_metrics = ['App_Login_Frequency', 'UPI_Transactions', 'Online_Banking_Activity']
        if all(metric in df.columns for metric in engagement_metrics):
            return df.withColumn(
                'Digital_Engagement_Score',
                (col('App_Login_Frequency') + col('UPI_Transactions') + col('Online_Banking_Activity')) / 3
            )
        return df

class AddressProcessor(Transformer):
    """Extract city and pincode from address column"""
    def __init__(self):
        super().__init__()
        self.singapore_regions = [
            'Tengah', 'Loyang', 'Pasir Ris', 'Tampines', 'Clementi', 'Jurong',
            'Queenstown', 'Woodlands', 'Serangoon', 'Bedok', 'Ang Mo Kio',
            'Toa Payoh', 'Bishan', 'Bukit Merah', 'Bukit Timah', 'Geylang',
            'Kallang', 'Marine Parade', 'Novena', 'Potong Pasir', 'Punggol',
            'Sembawang', 'Sengkang', 'Hougang', 'Yishun', 'Lim Chu Kang',
            'Mandai', 'Sungei Kadut', 'Central Area', 'Downtown Core',
            'Newton', 'Orchard', 'River Valley', 'Rochor', 'Singapore River',
            'Southern Islands', 'Straits View', 'Outram', 'Museum',
            'Dhoby Ghaut', 'Marina South', 'Marina East', 'Marina Centre'
        ]

    def _extract_city_udf(self):
        def extract_city(address):
            if address is None:
                return None
            address_lower = address.lower()
            for region in self.singapore_regions:
                if region.lower() in address_lower:
                    return region
            match = re.search(r'(\w+)\s+Singapore', address, re.IGNORECASE)
            if match:
                return match.group(1)
            return 'Unknown'
        return udf(extract_city, StringType())

    def _extract_pincode_udf(self):
        def extract_pincode(address):
            if address is None:
                return None
            pincode_pattern = r'\b(\d{6})\b'
            match = re.search(pincode_pattern, address)
            if match:
                return match.group(1)
            return None
        return udf(extract_pincode, StringType())

    def _transform(self, df):
        if 'Address' in df.columns:
            return df \
                .withColumn('City', self._extract_city_udf()(col('Address'))) \
                .withColumn('Pincode', self._extract_pincode_udf()(col('Address'))) \
                .drop('Address')
        return df

class PhoneNumberProcessor(Transformer):
    """Process and validate Singapore phone numbers"""
    def __init__(self):
        super().__init__()
        self.singapore_country_codes = ['+65', '65', '0065']

    def _process_phone_udf(self):
        def process_phone(phone):
            if phone is None or not isinstance(phone, str):
                return None
            cleaned = re.sub(r'[\s\-\(\)]', '', str(phone))
            for country_code in self.singapore_country_codes:
                if cleaned.startswith(country_code):
                    cleaned = cleaned[len(country_code):]
                    break
            return cleaned
        return udf(process_phone, StringType())

    def _validate_phone_udf(self):
        def validate_phone(phone):
            if phone is None or not isinstance(phone, str):
                return False
            return bool(re.match(r'^[3689]\d{7}$', phone))
        return udf(validate_phone, BooleanType())

    def _transform(self, df):
        phone_columns = ['Primary_Phone_Number', 'Secondary_Mobile_Number', 'Landline_Phone_Number']
        result_df = df
        for col_name in phone_columns:
            if col_name in df.columns:
                result_df = result_df \
                    .withColumn(col_name, self._process_phone_udf()(col(col_name))) \
                    .withColumn(f'{col_name}_Valid', self._validate_phone_udf()(col(col_name)))
        return result_df

class EmailValidator(Transformer):
    """Validate email addresses and extract domain information"""
    def __init__(self):
        super().__init__()
        self.legitimate_domains = [
            'gmail.com', 'yahoo.com', 'hotmail.com', 'outlook.com', 'live.com',
            'icloud.com', 'protonmail.com', 'zoho.com', 'aol.com', 'mail.com',
            'gmail.com.sg', 'yahoo.com.sg', 'hotmail.com.sg', 'singnet.com.sg',
            'starhub.net.sg', 'pacific.net.sg'
        ]
        self.disposable_domains = [
            'tempmail.com', '10minutemail.com', 'guerrillamail.com',
            'mailinator.com', 'yopmail.com', 'trashmail.com',
            'disposableemail.com', 'fakeinbox.com', 'temp-mail.org'
        ]

    def _validate_email_udf(self):
        def validate_email(email):
            if email is None or not isinstance(email, str):
                return False
            email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
            return bool(re.match(email_pattern, email))
        return udf(validate_email, BooleanType())

    def _extract_domain_udf(self):
        def extract_domain(email):
            if email is None or not isinstance(email, str) or '@' not in email:
                return None
            return email.split('@')[1].lower()
        return udf(extract_domain, StringType())

    def _is_legitimate_domain_udf(self):
        def is_legitimate_domain(domain):
            if domain is None:
                return False
            return domain in self.legitimate_domains
        return udf(is_legitimate_domain, BooleanType())

    def _is_disposable_domain_udf(self):
        def is_disposable_domain(domain):
            if domain is None:
                return False
            return domain in self.disposable_domains
        return udf(is_disposable_domain, BooleanType())

    def _transform(self, df):
        if 'Email_ID' in df.columns:
            return df \
                .withColumn('Email_Valid_Format', self._validate_email_udf()(col('Email_ID'))) \
                .withColumn('Email_Domain', self._extract_domain_udf()(col('Email_ID'))) \
                .withColumn('Email_Domain_Legitimate', self._is_legitimate_domain_udf()(col('Email_Domain'))) \
                .withColumn('Email_Disposable', self._is_disposable_domain_udf()(col('Email_Domain')))
        return df

class DataLoader(Transformer):
    """Load and initial data processing"""
    def __init__(self, file_path):
        super().__init__()
        self.file_path = file_path

    def _transform(self, df):
        print("Loading data...")
        try:
            df = spark.read.csv(self.file_path, header=True, inferSchema=True)
            print(f"Successfully loaded {df.count():,} rows")
            return df
        except Exception as e:
            print(f"Error loading data: {e}")
            return spark.createDataFrame([], StructType([]))

class PySparkDataPipeline:
    def __init__(self, file_path):
        self.file_path = file_path
        self.pipeline = None
        self.processed_data = None
        self.numeric_columns = []
        self.categorical_columns = []
        self._build_pipeline()

    def _get_numeric_columns(self, df):
        """Define numeric columns for preprocessing"""
        numeric_cols = ['Age', 'Loan_Amount_SGD', 'Outstanding_Balance_SGD', 'Day_Past_Due',
                       'Tenure', 'Interest_Rate', 'Current_EMI_SGD', 'Number_of_Past_Payments',
                       'Amount_Paid_Each_Month_SGD', 'Bounce_History', 'Contact_History_Call_Attempts',
                       'Contact_History_SMS', 'Contact_History_WhatsApp', 'Contact_History_EmailLogs',
                       'No_of_Attempts', 'Average_Handling_Time', 'Credit_Score', 'Recent_Inquiries',
                       'Loan_Exposure_Across_Banks', 'Recent_Score_Change', 'Unemployeement_rate_region',
                       'Inflation_Rate', 'Interest_Rate_Trend', 'Economic_Stress_Index',
                       'Income_Band_SGD', 'Digital_Engagement_Score']
        return [col for col in numeric_cols if col in df.columns]

    def _get_categorical_columns(self, df):
        """Define categorical columns for preprocessing"""
        categorical_cols = ['Product_Type', 'Payment_Frequency', 'Settlement_History',
                          'Channel_used', 'Response_Outcome', 'Urban_Rural_Tag',
                          'Language_Preference', 'Smartphone_Penetration', 'Preferred_Channel',
                          'Call_SMS_Activity_Patterns', 'WhatsApp_OTT_usage_Indicator',
                          'Regional_Time_Restrictions', 'Communication_Complaince_Limits',
                          'Gender', 'Occupation', 'Employeement_Type']
        return [col for col in categorical_cols if col in df.columns]

    def _get_date_columns(self):
        """Define date columns for processing"""
        return ['Installment_Due_Date', 'Last_Payment_Date']

    def _build_pipeline(self):
        """Build the PySpark pipeline"""
        self.pipeline = Pipeline(stages=[
            DataLoader(self.file_path),
            AddressProcessor(),
            PhoneNumberProcessor(),
            EmailValidator(),
            BooleanConverter(),
            IncomeBandEncoder(),
            DateFeatureExtractor(self._get_date_columns()),
            DigitalEngagementCalculator()
        ])

    def fit_transform(self, save_path=None):
        """Run the complete pipeline and return processed data"""
        print("=" * 50)
        print("RUNNING PYSPARK DATA PIPELINE")
        print("=" * 50)

        try:
            # Fit the pipeline
            pipeline_model = self.pipeline.fit(spark.createDataFrame([], StructType([])))
            
            # Transform the data
            self.processed_data = pipeline_model.transform(spark.createDataFrame([], StructType([])))
            
            # Cache the processed data for faster access
            self.processed_data.cache()
            
            # Get column lists after transformations
            self.numeric_columns = self._get_numeric_columns(self.processed_data)
            self.categorical_columns = self._get_categorical_columns(self.processed_data)

            # Apply additional preprocessing
            self._apply_standard_preprocessing()

            # Remove duplicates
            self.processed_data = self.processed_data.dropDuplicates()

            # Save if path provided
            if save_path:
                self.save_processed_data(save_path)

            print("Pipeline completed successfully!")
            print(f"Processed shape: ({self.processed_data.count():,}, {len(self.processed_data.columns)})")

            return self.processed_data

        except Exception as e:
            print(f"Error in pipeline: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _apply_standard_preprocessing(self):
        """Apply standard preprocessing to numeric and categorical columns"""
        # Handle numeric columns
        if self.numeric_columns:
            # Impute numeric columns
            numeric_imputer = Imputer(
                inputCols=self.numeric_columns,
                outputCols=[f"{col}_imputed" for col in self.numeric_columns],
                strategy="median"
            )
            
            # Scale numeric columns
            assembler = VectorAssembler(
                inputCols=[f"{col}_imputed" for col in self.numeric_columns],
                outputCol="numeric_features"
            )
            
            scaler = StandardScaler(
                inputCol="numeric_features",
                outputCol="scaled_features",
                withStd=True,
                withMean=True
            )
            
            # Create pipeline for numeric preprocessing
            numeric_pipeline = Pipeline(stages=[numeric_imputer, assembler, scaler])
            numeric_model = numeric_pipeline.fit(self.processed_data)
            self.processed_data = numeric_model.transform(self.processed_data)
            
            # Drop intermediate columns
            cols_to_drop = self.numeric_columns + [f"{col}_imputed" for col in self.numeric_columns] + ["numeric_features"]
            self.processed_data = self.processed_data.drop(*cols_to_drop)

        # Handle categorical columns
        if self.categorical_columns:
            # String index categorical columns
            indexers = [
                StringIndexer(inputCol=col, outputCol=f"{col}_indexed", handleInvalid="keep")
                for col in self.categorical_columns
            ]
            
            # One-hot encode indexed columns
            encoder = OneHotEncoder(
                inputCols=[f"{col}_indexed" for col in self.categorical_columns],
                outputCols=[f"{col}_encoded" for col in self.categorical_columns]
            )
            
            # Create pipeline for categorical preprocessing
            categorical_pipeline = Pipeline(stages=indexers + [encoder])
            categorical_model = categorical_pipeline.fit(self.processed_data)
            self.processed_data = categorical_model.transform(self.processed_data)
            
            # Drop intermediate columns
            cols_to_drop = self.categorical_columns + [f"{col}_indexed" for col in self.categorical_columns]
            self.processed_data = self.processed_data.drop(*cols_to_drop)

    def save_processed_data(self, path):
        """Save the processed data"""
        if self.processed_data is not None:
            self.processed_data.write.mode("overwrite").parquet(path)
            print(f"Processed data saved to {path}")

            # Also save the pipeline model for future use
            pipeline_model = self.pipeline.fit(spark.createDataFrame([], StructType([])))
            pipeline_model.write().overwrite().save("pyspark_data_pipeline_model")
            print("Pipeline model saved as 'pyspark_data_pipeline_model'")

    def load_and_transform_new_data(self, new_data_path):
        """Load and transform new data using the fitted pipeline"""
        # Load new data
        new_df = spark.read.csv(new_data_path, header=True, inferSchema=True)
        
        # Load the saved pipeline model
        from pyspark.ml import PipelineModel
        pipeline_model = PipelineModel.load("pyspark_data_pipeline_model")
        
        # Transform new data
        return pipeline_model.transform(new_df)

# Example usage
if __name__ == "__main__":
    # Initialize the PySpark pipeline
    pyspark_pipeline = PySparkDataPipeline('singapore_loan_data.csv')

    # Run the complete pipeline
    processed_data = pyspark_pipeline.fit_transform(save_path='processed_data_parquet')

    if processed_data is not None:
        # Display results
        print("\nPipeline Summary:")
        print(f"Processed data shape: ({processed_data.count():,}, {len(processed_data.columns)})")

        print("\nSample of processed data:")
        processed_data.show(5)

        print("\nData types after processing:")
        processed_data.printSchema()

        # Show the new columns created by the data cleaning processes
        new_columns = ['City', 'Pincode', 'Email_Valid_Format', 'Email_Domain',
                      'Email_Domain_Legitimate', 'Email_Disposable']

        print("\nNew columns created by data cleaning:")
        for col in new_columns:
            if col in processed_data.columns:
                non_null_count = processed_data.filter(col(col).isNotNull()).count()
                print(f"{col}: {non_null_count:,} non-null values")

        # Show phone number validation results
        phone_columns = ['Primary_Phone_Number', 'Secondary_Mobile_Number', 'Landline_Phone_Number']
        for col in phone_columns:
            valid_col = f'{col}_Valid'
            if valid_col in processed_data.columns:
                valid_count = processed_data.filter(col(valid_col) == True).count()
                total_count = processed_data.count()
                print(f"{valid_col}: {valid_count:,}/{total_count:,} valid numbers")
    else:
        print("Pipeline failed to process data.")

    # Stop Spark session
    spark.stop()
