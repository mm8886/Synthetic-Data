from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf, split, lit, monotonically_increasing_id
from pyspark.sql.types import *
import numpy as np
import pandas as pd
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.ml.linalg import Vectors
from xgboost.spark import SparkXGBRanker
from sklearn.metrics import ndcg_score
import joblib
import warnings
warnings.filterwarnings('ignore')

# ==============================================================
# Initialize Spark
# ==============================================================
spark = SparkSession.builder \
    .appName("XGBRanker_PySpark_Equivalent") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

# ==============================================================
# Data Preparation Class
# ==============================================================
class ChannelRankingDataPreparatorSpark:
    """Prepare ranking data in PySpark equivalent to pandas logic."""

    def __init__(self):
        self.channels = ['SMS', 'Email', 'Call', 'WhatsApp', 'IVR', 'Field_Agent']
        self.feature_cols = None
        self.label_encoders = {}

    def prepare_ranking_data(self, df_spark):
        print("Preparing ranking format...")

        # Collect schema information
        all_cols = df_spark.columns
        exclude_cols = ['Customer_id', 'Channel_Preference_Order', 'Preference_Label', 'Top_Channel']
        exclude_cols.extend([c for c in all_cols if 'Prefers_' in c])
        self.feature_cols = [c for c in all_cols if c not in exclude_cols]

        print(f"Selected {len(self.feature_cols)} feature columns.")

        # Encode categorical columns using StringIndexer
        categorical_cols = [f for (f, t) in df_spark.dtypes if t == 'string' and f in self.feature_cols]
        for col_name in categorical_cols:
            indexer = StringIndexer(inputCol=col_name, outputCol=f"{col_name}_idx", handleInvalid='keep')
            df_spark = indexer.fit(df_spark).transform(df_spark)
        # Replace feature columns with encoded
        self.feature_cols = [f"{c}_idx" if c in categorical_cols else c for c in self.feature_cols]

        print("Encoding completed for categorical columns.")

        # Convert to Pandas for group-wise ranking (because ranking logic is complex)
        pdf = df_spark.toPandas()
        print(f"Processing {len(pdf)} customers...")

        ranking_data = []
        group_sizes = []

        for idx, row in pdf.iterrows():
            customer_id = row['Customer_id']
            pref_order = str(row['Channel_Preference_Order']).split(',')
            customer_features = row[self.feature_cols].values.astype(float)

            for rank, channel in enumerate(pref_order):
                if channel in self.channels:
                    channel_features = np.zeros(len(self.channels))
                    ch_idx = self.channels.index(channel)
                    channel_features[ch_idx] = 1
                    combined_features = np.concatenate([customer_features, channel_features])
                    relevance = len(self.channels) - rank

                    ranking_data.append({
                        'customer_id': customer_id,
                        'channel': channel,
                        'features': Vectors.dense(combined_features),
                        'relevance': float(relevance),
                        'group_id': idx
                    })
            group_sizes.append(len(self.channels))

        df_rank = spark.createDataFrame(ranking_data)
        print(f"\nRanking dataset created:")
        print(f"Total samples: {df_rank.count():,}")
        print(f"Total customers (groups): {len(group_sizes):,}")
        print(f"Features per sample: {len(ranking_data[0]['features'])}")
        print(f"Group sizes: {group_sizes[0]} (should equal {len(self.channels)})")

        return df_rank, np.array(group_sizes)

# ==============================================================
# Train XGBRanker with PySpark
# ==============================================================
def train_xgb_ranker_spark(df_rank, groups, random_state=42):
    print("\n" + "="*60)
    print("TRAINING XGBRANKER MODEL (PySpark Equivalent)")
    print("="*60)

    # Split data manually like original
    n_train_groups = int(len(groups) * 0.8)
    train_groups = list(range(n_train_groups))
    test_groups = list(range(n_train_groups, len(groups)))

    df_train = df_rank.filter(col("group_id") < n_train_groups)
    df_test = df_rank.filter(col("group_id") >= n_train_groups)

    print(f"Train set: {df_train.count():,} samples, Test set: {df_test.count():,} samples")

    # Initialize Spark XGBRanker
    ranker = SparkXGBRanker(
        objective='rank:ndcg',
        eval_metric='ndcg@32',
        num_round=100,
        max_depth=6,
        eta=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=random_state,
        num_workers=2
    )

    # Fit model
    model = ranker.fit(df_train, group_col="group_id", label_col="relevance", features_col="features")
    print("\nModel training completed successfully!")

    # Predictions
    predictions = model.transform(df_test).select("group_id", "relevance", "prediction").toPandas()

    # Evaluate NDCG
    y_true, y_pred = [], []
    for gid in sorted(predictions['group_id'].unique()):
        grp = predictions[predictions['group_id'] == gid]
        y_true.append(grp['relevance'].values)
        y_pred.append(grp['prediction'].values)

    ndcg = ndcg_score(y_true, y_pred)
    print(f"NDCG Score: {ndcg:.4f}")

    return model, ndcg

# ==============================================================
# Demonstrate Predictions
# ==============================================================
def demonstrate_predictions_spark(model, df_rank, channels, n_customers=5):
    print("\n" + "="*60)
    print("EXAMPLE PREDICTIONS (PySpark)")
    print("="*60)

    samples = df_rank.filter(col("group_id") < n_customers).toPandas()

    for i in range(n_customers):
        cust = samples[samples['group_id'] == i]
        preds = model.transform(spark.createDataFrame(cust)).toPandas()
        preds = preds[['channel', 'prediction']].sort_values(by='prediction', ascending=False)

        print(f"\nCustomer {i+1} Channel Preferences:")
        for rank, row in enumerate(preds.itertuples(), 1):
            print(f"  {rank}. {row.channel}: {row.prediction:.4f}")

# ==============================================================
# MAIN EXECUTION
# ==============================================================
if __name__ == "__main__":
    print("="*60)
    print("PREPARING CHANNEL RANKING DATA FOR XGBRANKER (PySpark)")
    print("="*60)

    try:
        df_spark = spark.read.csv("features_with_labels.csv", header=True, inferSchema=True)
        print(f"Loaded data shape: {df_spark.count()} rows, {len(df_spark.columns)} columns")
    except Exception as e:
        print("Error loading data:", e)
        exit(1)

    preparator = ChannelRankingDataPreparatorSpark()
    df_rank, groups = preparator.prepare_ranking_data(df_spark)

    model, ndcg_val = train_xgb_ranker_spark(df_rank, groups)

    model.write().overwrite().save("xgb_channel_ranker_spark.model")
    joblib.dump(preparator, "channel_ranking_data_preparator_spark.pkl")

    demonstrate_predictions_spark(model, df_rank, preparator.channels)

    print("\n" + "="*60)
    print("MODEL TRAINING COMPLETED SUCCESSFULLY! (PySpark Version)")
    print("="*60)
    print(f"Final NDCG Score: {ndcg_val:.4f}")
