from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.ml import Pipeline
from pyspark.ml.feature import StandardScaler, VectorAssembler, StringIndexer, OneHotEncoder, Imputer
from pyspark.ml.base import Transformer, Estimator
from pyspark.ml.param.shared import HasInputCol, HasOutputCol, HasInputCols, HasOutputCols
from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable
import pyspark.sql.functions as F
from pyspark.sql import DataFrame
from typing import List, Optional, Dict, Any
import re
import os

class SparkDataPipeline:
    """Main PySpark Data Pipeline for Singapore Loan Data Processing"""
    
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.spark = SparkSession.builder \
            .appName("SingaporeLoanDataPipeline") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .getOrCreate()
        
        self.processed_data = None
        self.pipeline_model = None
        
        # Define column categories
        self.numeric_columns = [
            'Age', 'Loan_Amount_SGD', 'Outstanding_Balance_SGD', 'Day_Past_Due',
            'Tenure', 'Interest_Rate', 'Current_EMI_SGD', 'Number_of_Past_Payments',
            'Amount_Paid_Each_Month_SGD', 'Bounce_History', 'Contact_History_Call_Attempts',
            'Contact_History_SMS', 'Contact_History_WhatsApp', 'Contact_History_EmailLogs',
            'No_of_Attempts', 'Average_Handling_Time', 'Credit_Score', 'Recent_Inquiries',
            'Loan_Exposure_Across_Banks', 'Recent_Score_Change', 'Unemployeement_rate_region',
            'Inflation_Rate', 'Interest_Rate_Trend', 'Economic_Stress_Index'
        ]
        
        self.categorical_columns = [
            'Product_Type', 'Payment_Frequency', 'Settlement_History',
            'Channel_used', 'Response_Outcome', 'Urban_Rural_Tag',
            'Language_Preference', 'Smartphone_Penetration', 'Preferred_Channel',
            'Call_SMS_Activity_Patterns', 'WhatsApp_OTT_usage_Indicator',
            'Regional_Time_Restrictions', 'Communication_Complaince_Limits',
            'Gender', 'Occupation', 'Employeement_Type'
        ]
        
        self.date_columns = ['Installment_Due_Date', 'Last_Payment_Date']
        
        self.boolean_columns = [
            'Partial_Payment_Indicator', 'Repayment_Irregularity_Flags',
            'Mobile_Number_Active_Status', 'Email_Activity', 'Do_Not_Call_Registry_Data'
        ]

    def load_data(self) -> DataFrame:
        """Load data from CSV file"""
        print("Loading data...")
        
        try:
            df = self.spark.read \
                .option("header", "true") \
                .option("inferSchema", "true") \
                .csv(self.file_path)
            
            print(f"Successfully loaded {df.count():,} rows")
            return df
            
        except Exception as e:
            print(f"Error loading data: {e}")
            raise e

class AddressProcessor(Transformer, HasInputCol, HasOutputCols, DefaultParamsReadable, DefaultParamsWritable):
    """Extract city and pincode from address column"""
    
    def __init__(self, inputCol: str = "Address", outputCols: List[str] = None):
        super(AddressProcessor, self).__init__()
        self._set(inputCol=inputCol)
        if outputCols is None:
            outputCols = ["City", "Pincode"]
        self._set(outputCols=outputCols)
        
        # Singapore regions
        self.singapore_regions = [
            'Tengah', 'Loyang', 'Pasir Ris', 'Tampines', 'Clementi', 'Jurong',
            'Queenstown', 'Woodlands', 'Serangoon', 'Bedok', 'Ang Mo Kio',
            'Toa Payoh', 'Bishan', 'Bukit Merah', 'Bukit Timah', 'Geylang',
            'Kallang', 'Marine Parade', 'Novena', 'Potong Pasir', 'Punggol',
            'Sembawang', 'Sengkang', 'Hougang', 'Yishun', 'Lim Chu Kang',
            'Mandai', 'Sungei Kadut', 'Central Area', 'Downtown Core',
            'Newton', 'Orchard', 'River Valley', 'Rochor', 'Singapore River',
            'Southern Islands', 'Straits View', 'Outram', 'Museum',
            'Dhoby Ghaut', 'Marina South', 'Marina East', 'Marina Centre'
        ]

    def _extract_city_udf(self):
        """UDF to extract city from address"""
        def extract_city(address):
            if address is None:
                return "Unknown"
            
            address_str = str(address).lower()
            for region in self.singapore_regions:
                if region.lower() in address_str:
                    return region
            
            # Fallback: extract word before Singapore
            match = re.search(r'(\w+)\s+Singapore', address, re.IGNORECASE)
            if match:
                return match.group(1)
            
            return "Unknown"
        
        return udf(extract_city, StringType())

    def _extract_pincode_udf(self):
        """UDF to extract pincode from address"""
        def extract_pincode(address):
            if address is None:
                return None
            
            pincode_pattern = r'\b(\d{6})\b'
            match = re.search(pincode_pattern, str(address))
            return match.group(1) if match else None
        
        return udf(extract_pincode, StringType())

    def _transform(self, df: DataFrame) -> DataFrame:
        input_col = self.getInputCol()
        output_cols = self.getOutputCols()
        
        if input_col not in df.columns:
            return df
            
        city_udf = self._extract_city_udf()
        pincode_udf = self._extract_pincode_udf()
        
        df = df \
            .withColumn(output_cols[0], city_udf(col(input_col))) \
            .withColumn(output_cols[1], pincode_udf(col(input_col))) \
            .drop(input_col)
            
        return df

class PhoneNumberProcessor(Transformer, HasInputCols, HasOutputCols, DefaultParamsReadable, DefaultParamsWritable):
    """Process and validate Singapore phone numbers"""
    
    def __init__(self, inputCols: List[str] = None, outputCols: List[str] = None):
        super(PhoneNumberProcessor, self).__init__()
        if inputCols is None:
            inputCols = ['Primary_Phone_Number', 'Secondary_Mobile_Number', 'Landline_Phone_Number']
        if outputCols is None:
            outputCols = [f'{col}_Valid' for col in inputCols]
        
        self._set(inputCols=inputCols)
        self._set(outputCols=outputCols)
        self.singapore_country_codes = ['+65', '65', '0065']

    def _process_phone_udf(self):
        """UDF to process phone numbers"""
        def process_phone(phone):
            if phone is None:
                return None
            
            cleaned = re.sub(r'[\s\-\(\)]', '', str(phone))
            
            for country_code in self.singapore_country_codes:
                if cleaned.startswith(country_code):
                    cleaned = cleaned[len(country_code):]
                    break
            
            return cleaned
        
        return udf(process_phone, StringType())

    def _validate_phone_udf(self):
        """UDF to validate Singapore phone numbers"""
        def validate_phone(phone):
            if phone is None:
                return False
            
            phone_str = str(phone)
            return bool(re.match(r'^[3689]\d{7}$', phone_str))
        
        return udf(validate_phone, BooleanType())

    def _transform(self, df: DataFrame) -> DataFrame:
        input_cols = self.getInputCols()
        output_cols = self.getOutputCols()
        
        process_udf = self._process_phone_udf()
        validate_udf = self._validate_phone_udf()
        
        for input_col, output_col in zip(input_cols, output_cols):
            if input_col in df.columns:
                df = df \
                    .withColumn(input_col, process_udf(col(input_col))) \
                    .withColumn(output_col, validate_udf(col(input_col)))
        
        return df

class EmailValidator(Transformer, HasInputCol, HasOutputCols, DefaultParamsReadable, DefaultParamsWritable):
    """Validate email addresses and extract domain information"""
    
    def __init__(self, inputCol: str = "Email_ID", outputCols: List[str] = None):
        super(EmailValidator, self).__init__()
        self._set(inputCol=inputCol)
        if outputCols is None:
            outputCols = ["Email_Valid_Format", "Email_Domain", "Email_Domain_Legitimate", "Email_Disposable"]
        self._set(outputCols=outputCols)
        
        self.legitimate_domains = [
            'gmail.com', 'yahoo.com', 'hotmail.com', 'outlook.com', 'live.com',
            'icloud.com', 'protonmail.com', 'zoho.com', 'aol.com', 'mail.com',
            'gmail.com.sg', 'yahoo.com.sg', 'hotmail.com.sg', 'singnet.com.sg',
            'starhub.net.sg', 'pacific.net.sg'
        ]
        
        self.disposable_domains = [
            'tempmail.com', '10minutemail.com', 'guerrillamail.com',
            'mailinator.com', 'yopmail.com', 'trashmail.com',
            'disposableemail.com', 'fakeinbox.com', 'temp-mail.org'
        ]

    def _validate_email_udf(self):
        """UDF to validate email format"""
        def validate_email(email):
            if email is None:
                return False
            
            email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
            return bool(re.match(email_pattern, str(email)))
        
        return udf(validate_email, BooleanType())

    def _extract_domain_udf(self):
        """UDF to extract domain from email"""
        def extract_domain(email):
            if email is None or '@' not in str(email):
                return None
            
            return str(email).split('@')[1].lower()
        
        return udf(extract_domain, StringType())

    def _is_legitimate_domain_udf(self):
        """UDF to check if domain is legitimate"""
        def is_legitimate(domain):
            if domain is None:
                return False
            return domain in self.legitimate_domains
        
        return udf(is_legitimate, BooleanType())

    def _is_disposable_domain_udf(self):
        """UDF to check if domain is disposable"""
        def is_disposable(domain):
            if domain is None:
                return False
            return domain in self.disposable_domains
        
        return udf(is_disposable, BooleanType())

    def _transform(self, df: DataFrame) -> DataFrame:
        input_col = self.getInputCol()
        output_cols = self.getOutputCols()
        
        if input_col not in df.columns:
            return df
            
        validate_udf = self._validate_email_udf()
        domain_udf = self._extract_domain_udf()
        legitimate_udf = self._is_legitimate_domain_udf()
        disposable_udf = self._is_disposable_domain_udf()
        
        df = df \
            .withColumn(output_cols[0], validate_udf(col(input_col))) \
            .withColumn(output_cols[1], domain_udf(col(input_col))) \
            .withColumn(output_cols[2], legitimate_udf(col(output_cols[1]))) \
            .withColumn(output_cols[3], disposable_udf(col(output_cols[1])))
        
        return df

class BooleanConverter(Transformer, HasInputCols, DefaultParamsReadable, DefaultParamsWritable):
    """Convert various boolean representations to proper booleans"""
    
    def __init__(self, inputCols: List[str] = None):
        super(BooleanConverter, self).__init__()
        if inputCols is None:
            inputCols = [
                'Partial_Payment_Indicator', 'Repayment_Irregularity_Flags',
                'Mobile_Number_Active_Status', 'Email_Activity', 'Do_Not_Call_Registry_Data'
            ]
        self._set(inputCols=inputCols)

    def _convert_boolean_udf(self):
        """UDF to convert various boolean representations"""
        def convert_boolean(value):
            if value is None:
                return False
            
            str_val = str(value).lower()
            if str_val in ['true', '1', 'yes']:
                return True
            elif str_val in ['false', '0', 'no']:
                return False
            else:
                return bool(value)
        
        return udf(convert_boolean, BooleanType())

    def _transform(self, df: DataFrame) -> DataFrame:
        input_cols = self.getInputCols()
        convert_udf = self._convert_boolean_udf()
        
        for col_name in input_cols:
            if col_name in df.columns:
                df = df.withColumn(col_name, convert_udf(col(col_name)))
        
        return df

class IncomeBandEncoder(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):
    """Encode income bands to numerical values"""
    
    def __init__(self, inputCol: str = "Income_Band_SGD", outputCol: str = "Income_Band_Encoded"):
        super(IncomeBandEncoder, self).__init__()
        self._set(inputCol=inputCol)
        self._set(outputCol=outputCol)
        
        self.income_mapping = {
            '50,000 or Below': 0,
            '50,000 to 100,000': 1,
            '100,000 to 200,000': 2,
            '200,000 to 300,000': 3,
            '300,000 to 500,000': 4,
            '500,000 or Above': 5
        }

    def _encode_income_udf(self):
        """UDF to encode income bands"""
        def encode_income(income_band):
            if income_band is None:
                return None
            return self.income_mapping.get(income_band, None)
        
        return udf(encode_income, IntegerType())

    def _transform(self, df: DataFrame) -> DataFrame:
        input_col = self.getInputCol()
        output_col = self.getOutputCol()
        
        if input_col not in df.columns:
            return df
            
        encode_udf = self._encode_income_udf()
        return df.withColumn(output_col, encode_udf(col(input_col)))

class DateFeatureExtractor(Transformer, HasInputCols, DefaultParamsReadable, DefaultParamsWritable):
    """Extract features from date columns"""
    
    def __init__(self, inputCols: List[str] = None):
        super(DateFeatureExtractor, self).__init__()
        if inputCols is None:
            inputCols = ['Installment_Due_Date', 'Last_Payment_Date']
        self._set(inputCols=inputCols)

    def _transform(self, df: DataFrame) -> DataFrame:
        input_cols = self.getInputCols()
        
        for col_name in input_cols:
            if col_name in df.columns:
                df = df \
                    .withColumn(col_name, to_date(col(col_name), "yyyy-MM-dd")) \
                    .withColumn(f"{col_name}_month", month(col(col_name))) \
                    .withColumn(f"{col_name}_dayofweek", dayofweek(col(col_name))) \
                    .withColumn(f"{col_name}_year", year(col(col_name))) \
                    .drop(col_name)
        
        return df

class DigitalEngagementCalculator(Transformer, DefaultParamsReadable, DefaultParamsWritable):
    """Calculate digital engagement score"""
    
    def __init__(self):
        super(DigitalEngagementCalculator, self).__init__()

    def _transform(self, df: DataFrame) -> DataFrame:
        engagement_metrics = ['App_Login_Frequency', 'UPI_Transactions', 'Online_Banking_Activity']
        
        if all(metric in df.columns for metric in engagement_metrics):
            df = df.withColumn(
                'Digital_Engagement_Score',
                (col('App_Login_Frequency') + col('UPI_Transactions') + col('Online_Banking_Activity')) / 3
            )
        
        return df

class PySparkDataPipeline(SparkDataPipeline):
    """Enhanced PySpark Data Pipeline with complete preprocessing"""
    
    def __init__(self, file_path: str):
        super().__init__(file_path)
        
    def build_pipeline(self) -> Pipeline:
        """Build the complete PySpark ML pipeline"""
        
        # Define stages
        stages = []
        
        # 1. Address Processing
        stages.append(AddressProcessor())
        
        # 2. Phone Number Processing
        stages.append(PhoneNumberProcessor())
        
        # 3. Email Validation
        stages.append(EmailValidator())
        
        # 4. Boolean Conversion
        stages.append(BooleanConverter())
        
        # 5. Income Band Encoding
        stages.append(IncomeBandEncoder())
        
        # 6. Date Feature Extraction
        stages.append(DateFeatureExtractor())
        
        # 7. Digital Engagement Calculation
        stages.append(DigitalEngagementCalculator())
        
        # 8. Handle missing values for numeric columns
        numeric_imputer = Imputer(
            inputCols=[col for col in self.numeric_columns if col != 'Income_Band_SGD'],
            outputCols=[f"{col}_imputed" for col in self.numeric_columns if col != 'Income_Band_SGD'],
            strategy="median"
        )
        stages.append(numeric_imputer)
        
        # 9. Standard scaling for numeric columns
        numeric_assembler = VectorAssembler(
            inputCols=[f"{col}_imputed" for col in self.numeric_columns if col != 'Income_Band_SGD'],
            outputCol="numeric_features"
        )
        stages.append(numeric_assembler)
        
        scaler = StandardScaler(
            inputCol="numeric_features",
            outputCol="scaled_numeric_features",
            withStd=True,
            withMean=True
        )
        stages.append(scaler)
        
        # 10. Categorical encoding
        for categorical_col in self.categorical_columns:
            if categorical_col in self.categorical_columns:  # Double check
                indexer = StringIndexer(
                    inputCol=categorical_col,
                    outputCol=f"{categorical_col}_indexed",
                    handleInvalid="keep"
                )
                encoder = OneHotEncoder(
                    inputCol=f"{categorical_col}_indexed",
                    outputCol=f"{categorical_col}_encoded"
                )
                stages.extend([indexer, encoder])
        
        return Pipeline(stages=stages)

    def fit_transform(self, save_path: Optional[str] = None) -> DataFrame:
        """Run the complete pipeline and return processed data"""
        print("=" * 50)
        print("RUNNING PYSPARK DATA PIPELINE")
        print("=" * 50)
        
        try:
            # Load data
            df = self.load_data()
            original_count = df.count()
            original_columns = len(df.columns)
            
            print("Building and applying pipeline...")
            pipeline = self.build_pipeline()
            self.pipeline_model = pipeline.fit(df)
            self.processed_data = self.pipeline_model.transform(df)
            
            # Remove duplicates
            self.processed_data = self.processed_data.dropDuplicates()
            processed_count = self.processed_data.count()
            
            # Save if path provided
            if save_path:
                self.save_processed_data(save_path)
            
            print("Pipeline completed successfully!")
            print(f"Original shape: ({original_count:,} rows, {original_columns} columns)")
            print(f"Processed shape: ({processed_count:,} rows, {len(self.processed_data.columns)} columns)")
            
            return self.processed_data
            
        except Exception as e:
            print(f"Error in pipeline: {e}")
            import traceback
            traceback.print_exc()
            return None

    def save_processed_data(self, path: str):
        """Save the processed data"""
        if self.processed_data is not None:
            self.processed_data.write \
                .mode("overwrite") \
                .option("header", "true") \
                .csv(path)
            
            print(f"Processed data saved to {path}")
            
            # Save pipeline model
            self.pipeline_model.write().overwrite().save("pyspark_pipeline_model")
            print("Pipeline model saved as 'pyspark_pipeline_model'")

    def transform_new_data(self, new_data_path: str) -> DataFrame:
        """Transform new data using the fitted pipeline"""
        if self.pipeline_model is None:
            raise ValueError("Pipeline not fitted yet. Call fit_transform first.")
        
        # Load new data
        new_df = self.spark.read \
            .option("header", "true") \
            .option("inferSchema", "true") \
            .csv(new_data_path)
        
        # Transform using fitted pipeline
        return self.pipeline_model.transform(new_df)

    def show_pipeline_summary(self):
        """Display pipeline summary and results"""
        if self.processed_data is None:
            print("No processed data available. Run fit_transform first.")
            return
        
        print("\nPipeline Summary:")
        print(f"Processed data shape: ({self.processed_data.count():,} rows, {len(self.processed_data.columns)} columns)")
        
        print("\nSample of processed data:")
        self.processed_data.show(5)
        
        print("\nData types after processing:")
        self.processed_data.dtypes
        
        # Show new columns created
        new_columns = ['City', 'Pincode', 'Email_Valid_Format', 'Email_Domain',
                      'Email_Domain_Legitimate', 'Email_Disposable']
        
        print("\nNew columns created by data cleaning:")
        for col_name in new_columns:
            if col_name in self.processed_data.columns:
                non_null_count = self.processed_data.filter(col(col_name).isNotNull()).count()
                print(f"{col_name}: {non_null_count:,} non-null values")
        
        # Show phone number validation results
        phone_columns = ['Primary_Phone_Number', 'Secondary_Mobile_Number', 'Landline_Phone_Number']
        for col_name in phone_columns:
            valid_col = f'{col_name}_Valid'
            if valid_col in self.processed_data.columns:
                valid_count = self.processed_data.filter(col(valid_col) == True).count()
                total_count = self.processed_data.filter(col(valid_col).isNotNull()).count()
                print(f"{valid_col}: {valid_count:,}/{total_count:,} valid numbers")

# Example usage
if __name__ == "__main__":
    # Initialize the PySpark pipeline
    pyspark_pipeline = PySparkDataPipeline('singapore_loan_data.csv')
    
    # Run the complete pipeline
    processed_data = pyspark_pipeline.fit_transform(save_path='processed_data')
    
    if processed_data is not None:
        # Display results
        pyspark_pipeline.show_pipeline_summary()
        
        # Stop Spark session
        pyspark_pipeline.spark.stop()
    else:
        print("Pipeline failed to process data.")