from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder
from pyspark.ml import Pipeline
from pyspark.ml.linalg import Vectors, VectorUDT
import numpy as np
from sklearn.metrics import ndcg_score
import xgboost as xgb
import joblib
import warnings
warnings.filterwarnings('ignore')

class ChannelRankingDataPreparatorSpark:
    """Prepare data for XGBRanker training using PySpark"""

    def __init__(self):
        self.channels = ['SMS', 'Email', 'Call', 'WhatsApp', 'IVR', 'Field_Agent']
        self.feature_cols = None
        self.string_indexers = {}
        self.encoder_models = {}

    def prepare_ranking_data(self, df):
        """Convert preference data to ranking format using PySpark"""
        print("Preparing ranking format with PySpark...")
        
        # Get feature columns (exclude label columns and Customer_id)
        exclude_cols = ['Customer_id', 'Channel_Preference_Order', 'Preference_Label', 'Top_Channel']
        exclude_cols.extend([col for col in df.columns if 'Prefers_' in col])
        
        self.feature_cols = [col for col in df.columns if col not in exclude_cols]
        
        # Identify categorical columns
        categorical_cols = []
        for col in self.feature_cols:
            dtype = dict(df.dtypes)[col]
            if dtype in ['string', 'varchar']:
                categorical_cols.append(col)
        
        # Prepare data for processing
        spark_df = df.select(*self.feature_cols, 'Customer_id', 'Channel_Preference_Order')
        
        # Process each customer to create ranking dataset
        print(f"Processing {spark_df.count()} customers...")
        
        def create_ranking_samples(row):
            """Create ranking samples for each customer"""
            customer_features = []
            for col in self.feature_cols:
                val = row[col]
                # Convert to float, handling different data types
                if val is None:
                    customer_features.append(0.0)
                elif isinstance(val, (int, float)):
                    customer_features.append(float(val))
                else:
                    # For string categoricals, we'll index them later
                    customer_features.append(float(hash(str(val)) % 10000))
            
            customer_id = row['Customer_id']
            preference_order = row['Channel_Preference_Order'].split(',')
            
            samples = []
            for rank, channel in enumerate(preference_order):
                if channel in self.channels:
                    # Channel one-hot encoding
                    channel_features = [0.0] * len(self.channels)
                    channel_idx = self.channels.index(channel)
                    channel_features[channel_idx] = 1.0
                    
                    # Combine customer and channel features
                    combined_features = customer_features + channel_features
                    
                    # Relevance score (higher rank = higher relevance)
                    relevance = len(self.channels) - rank
                    
                    samples.append((
                        customer_id,
                        channel,
                        Vectors.dense(combined_features),
                        float(relevance),
                        int(hash(customer_id) % 1000000)  # group_id
                    ))
            
            return samples

        # Create ranking samples
        sample_rdd = spark_df.rdd.flatMap(create_ranking_samples)
        
        # Define schema for the result DataFrame
        schema = StructType([
            StructField("customer_id", StringType(), True),
            StructField("channel", StringType(), True),
            StructField("features", VectorUDT(), True),
            StructField("relevance", DoubleType(), True),
            StructField("group_id", IntegerType(), True)
        ])
        
        ranking_df = spark_df.sql_ctx.createDataFrame(sample_rdd, schema)
        
        # Cache the DataFrame for faster processing
        ranking_df.cache()
        
        # Convert to numpy arrays for XGBoost
        print("Converting to numpy arrays...")
        
        # Collect data and convert to numpy arrays
        collected_data = ranking_df.collect()
        
        X_ranking = np.array([row['features'].toArray() for row in collected_data])
        y_ranking = np.array([row['relevance'] for row in collected_data])
        
        # Create groups array
        group_df = ranking_df.groupBy("group_id").count().orderBy("group_id")
        groups = np.array([row['count'] for row in group_df.collect()])
        
        print(f"\nRanking dataset created:")
        print(f"Total samples: {len(X_ranking):,}")
        print(f"Total customers (groups): {len(groups):,}")
        print(f"Features per sample: {X_ranking.shape[1]}")
        print(f"Group sizes (samples per customer): {groups[0]} (all should be {len(self.channels)})")
        
        return X_ranking, y_ranking, groups, ranking_df

def train_xgb_ranker(X, y, groups, test_size=0.2, random_state=42):
    """Train XGBRanker model (same as original function)"""
    print("\n" + "="*60)
    print("TRAINING XGBRANKER MODEL")
    print("="*60)

    # Calculate split points for groups
    n_train_groups = int(len(groups) * (1 - test_size))
    train_samples = sum(groups[:n_train_groups])

    # Split data
    X_train = X[:train_samples]
    y_train = y[:train_samples]
    groups_train = groups[:n_train_groups]

    X_test = X[train_samples:]
    y_test = y[train_samples:]
    groups_test = groups[n_train_groups:]

    print(f"Feature matrix shape: {X_train.shape}")
    print(f"Label vector shape: {y_train.shape}")
    print(f"Number of groups: {len(groups_train):,}")
    print(f"\nTrain set: {len(X_train):,} samples from {len(groups_train):,} customers")
    print(f"Test set: {len(X_test):,} samples from {len(groups_test):,} customers")

    # Initialize XGBRanker
    print("\nInitializing XGBRanker...")
    model = xgb.XGBRanker(
        objective='rank:ndcg',
        learning_rate=0.1,
        max_depth=6,
        n_estimators=100,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=random_state,
        eval_metric='ndcg@32'
    )

    # Train model
    print("Training XGBRanker...")
    model.fit(
        X_train, y_train,
        group=groups_train,
        eval_set=[(X_test, y_test)],
        eval_group=[groups_test],
        verbose=True
    )

    print("\nModel training completed successfully!")

    # Make predictions
    print("\nMaking predictions...")
    y_pred = model.predict(X_test)

    # Calculate NDCG
    print("Calculating NDCG score...")
    # Reshape predictions for NDCG calculation
    y_test_reshaped = []
    y_pred_reshaped = []

    start_idx = 0
    for group_size in groups_test:
        end_idx = start_idx + group_size
        y_test_reshaped.append(y_test[start_idx:end_idx])
        y_pred_reshaped.append(y_pred[start_idx:end_idx])
        start_idx = end_idx

    ndcg = ndcg_score(y_test_reshaped, y_pred_reshaped)
    print(f"NDCG Score: {ndcg:.4f}")

    return model, ndcg

def demonstrate_predictions(model, X, channels, n_customers=5):
    """Show example predictions (same as original function)"""
    print("\n" + "="*60)
    print("EXAMPLE PREDICTIONS")
    print("="*60)

    samples_per_customer = len(channels)

    for i in range(n_customers):
        start_idx = i * samples_per_customer
        end_idx = start_idx + samples_per_customer

        customer_samples = X[start_idx:end_idx]
        predictions = model.predict(customer_samples)

        # Sort channels by prediction score
        channel_scores = list(zip(channels, predictions))
        channel_scores.sort(key=lambda x: x[1], reverse=True)

        print(f"\nCustomer {i+1} Channel Preferences:")
        for rank, (channel, score) in enumerate(channel_scores, 1):
            print(f"  {rank}. {channel}: {score:.4f}")

# Main execution
if __name__ == "__main__":
    print("="*60)
    print("PREPARING CHANNEL RANKING DATA FOR XGBRANKER WITH PYSPARK")
    print("="*60)

    # Initialize Spark session
    spark = SparkSession.builder \
        .appName("ChannelRanking") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .getOrCreate()

    # Load data
    print("Loading features_with_labels.csv...")
    try:
        df = spark.read.option("header", "true").option("inferSchema", "true").csv('features_with_labels.csv')
        print(f"Loaded data shape: ({df.count()}, {len(df.columns)})")
    except Exception as e:
        print(f"Error loading data: {e}")
        spark.stop()
        exit(1)

    # Prepare data
    preparator = ChannelRankingDataPreparatorSpark()
    X, y, groups, ranking_df = preparator.prepare_ranking_data(df)

    # Show some stats about the ranking DataFrame
    print(f"\nRanking DataFrame stats:")
    ranking_df.select(
        count("customer_id").alias("total_samples"),
        countDistinct("customer_id").alias("unique_customers"),
        countDistinct("group_id").alias("unique_groups")
    ).show()

    # Train model (using same function as it works with numpy arrays)
    model, ndcg_score = train_xgb_ranker(X, y, groups)

    # Save model and preparator
    model_filename = 'xgb_channel_ranker.pkl'
    joblib.dump(model, model_filename)
    joblib.dump(preparator, 'channel_ranking_data_preparator.pkl')
    print(f"\nModel saved as '{model_filename}'")
    print("Preparator saved as 'channel_ranking_data_preparator.pkl'")

    # Demonstrate predictions
    demonstrate_predictions(model, X, preparator.channels)

    print("\n" + "="*60)
    print("MODEL TRAINING COMPLETED SUCCESSFULLY!")
    print("="*60)
    print(f"Feature columns used: {X.shape[1]}")
    print(f"Model saved as: {model_filename}")
    print(f"Total customers processed: {len(groups):,}")
    print(f"Final NDCG Score: {ndcg_score:.4f}")

    # Stop Spark session
    spark.stop()
