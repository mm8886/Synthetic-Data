import re
from datetime import datetime

from pyspark.ml import Pipeline, Transformer
from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params
from pyspark.ml.feature import Imputer, StandardScaler, OneHotEncoder, StringIndexer, VectorAssembler
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import col, udf, when, month, dayofweek, year, to_timestamp
from pyspark.sql.types import StringType, BooleanType, IntegerType, DoubleType, ArrayType

# ==============================================================================
# 1. Custom Transformers for PySpark ML Pipeline
#
# Each class inherits from Transformer and includes mixins for input/output columns.
# The core logic is in the _transform method, which operates on a Spark DataFrame.
# ==============================================================================

class DateFeatureExtractor(Transformer, HasInputCol, HasOutputCol):
    """Extracts month, dayofweek, and year from date columns."""
    
    def __init__(self, inputCols=None, outputSuffix="_features"):
        super(DateFeatureExtractor, self).__init__()
        self.inputCols = inputCols
        self.outputSuffix = outputSuffix

    def _transform(self, df: DataFrame) -> DataFrame:
        df_transformed = df
        for date_col in self.inputCols:
            # Convert string to timestamp
            df_transformed = df_transformed.withColumn(f"{date_col}_ts", to_timestamp(col(date_col)))
            
            # Extract features
            df_transformed = df_transformed.withColumn(f"{date_col}_month", month(col(f"{date_col}_ts")))
            df_transformed = df_transformed.withColumn(f"{date_col}_dayofweek", dayofweek(col(f"{date_col}_ts")))
            df_transformed = df_transformed.withColumn(f"{date_col}_year", year(col(f"{date_col}_ts")))
            
            # Drop original and intermediate columns
            df_transformed = df_transformed.drop(date_col, f"{date_col}_ts")
            
        return df_transformed

class IncomeBandEncoder(Transformer, HasInputCol, HasOutputCol):
    """Encodes income bands to numerical values."""
    
    def __init__(self, inputCol="Income_Band_SGD", outputCol="Income_Band_SGD_encoded"):
        super(IncomeBandEncoder, self).__init__()
        self.setInputCol(inputCol)
        self.setOutputCol(outputCol)

    def _transform(self, df: DataFrame) -> DataFrame:
        return df.withColumn(
            self.getOutputCol(),
            when(col(self.getInputCol()) == '50,000 or Below', 0)
            .when(col(self.getInputCol()) == '50,000 to 100,000', 1)
            .when(col(self.getInputCol()) == '100,000 to 200,000', 2)
            .when(col(self.getInputCol()) == '200,000 to 300,000', 3)
            .when(col(self.getInputCol()) == '300,000 to 500,000', 4)
            .when(col(self.getInputCol()) == '500,000 or Above', 5)
            .otherwise(None) # Handle null or unknown values
        ).drop(self.getInputCol())

class BooleanConverter(Transformer):
    """Converts various boolean string representations to proper booleans."""
    
    def __init__(self, inputCols=None):
        super(BooleanConverter, self).__init__()
        self.inputCols = inputCols

    def _transform(self, df: DataFrame) -> DataFrame:
        df_transformed = df
        for bool_col in self.inputCols:
            df_transformed = df_transformed.withColumn(
                bool_col,
                when(col(bool_col).cast("string").rlike("(?i)true|1|yes"), True)
                .when(col(bool_col).cast("string").rlike("(?i)false|0|no"), False)
                .otherwise(None).cast(BooleanType())
            )
        return df_transformed

class DigitalEngagementCalculator(Transformer, HasOutputCol):
    """Calculates a digital engagement score."""
    
    def __init__(self, outputCol="Digital_Engagement_Score"):
        super(DigitalEngagementCalculator, self).__init__()
        self.setOutputCol(outputCol)
        self.engagement_metrics = ['App_Login_Frequency', 'UPI_Transactions', 'Online_Banking_Activity']

    def _transform(self, df: DataFrame) -> DataFrame:
        return df.withColumn(
            self.getOutputCol(),
            (col(self.engagement_metrics[0]) + col(self.engagement_metrics[1]) + col(self.engagement_metrics[2])) / 3.0
        )

class AddressProcessor(Transformer, HasInputCol):
    """Extracts city and pincode from address column using UDFs."""
    
    def __init__(self, inputCol="Address"):
        super(AddressProcessor, self).__init__()
        self.setInputCol(inputCol)

    def _transform(self, df: DataFrame) -> DataFrame:
        
        # Define UDFs for complex string manipulation
        @udf(StringType())
        def _extract_city(address):
            if address is None: return None
            singapore_regions = [
                'Tengah', 'Loyang', 'Pasir Ris', 'Tampines', 'Clementi', 'Jurong',
                'Queenstown', 'Woodlands', 'Serangoon', 'Bedok', 'Ang Mo Kio',
                'Toa Payoh', 'Bishan', 'Bukit Merah', 'Bukit Timah', 'Geylang',
                'Kallang', 'Marine Parade', 'Novena', 'Potong Pasir', 'Punggol',
                'Sembawang', 'Sengkang', 'Hougang', 'Yishun', 'Lim Chu Kang'
            ]
            address_lower = address.lower()
            for region in singapore_regions:
                if region.lower() in address_lower:
                    return region
            match = re.search(r'(\w+)\s+Singapore', address, re.IGNORECASE)
            return match.group(1) if match else 'Unknown'

        @udf(StringType())
        def _extract_pincode(address):
            if address is None: return None
            match = re.search(r'\b(\d{6})\b', address)
            return match.group(1) if match else None

        return df.withColumn("City", _extract_city(col(self.getInputCol()))) \
                 .withColumn("Pincode", _extract_pincode(col(self.getInputCol()))) \
                 .drop(self.getInputCol())

class PhoneNumberProcessor(Transformer):
    """Processes and validates Singapore phone numbers."""

    def __init__(self, inputCols=None):
        super(PhoneNumberProcessor, self).__init__()
        self.inputCols = inputCols
    
    def _transform(self, df: DataFrame) -> DataFrame:
        
        @udf(StringType())
        def _process_phone_number(phone):
            if phone is None: return None
            cleaned = re.sub(r'[\s\-\(\)]', '', str(phone))
            singapore_country_codes = ['+65', '65', '0065']
            for code in singapore_country_codes:
                if cleaned.startswith(code):
                    return cleaned[len(code):]
            return cleaned

        @udf(BooleanType())
        def _validate_singapore_number(phone):
            if phone is None: return False
            return bool(re.match(r'^[3689]\d{7}$', str(phone)))

        df_transformed = df
        for col_name in self.inputCols:
            # Clean the phone number
            processed_col_name = f"{col_name}_processed"
            df_transformed = df_transformed.withColumn(processed_col_name, _process_phone_number(col(col_name)))
            
            # Create a validation flag based on the cleaned number
            df_transformed = df_transformed.withColumn(f"{col_name}_Valid", _validate_singapore_number(col(processed_col_name)))
            df_transformed = df_transformed.drop(col_name) # Drop original
        return df_transformed


class EmailValidator(Transformer, HasInputCol):
    """Validates email addresses and extracts domain information."""

    def __init__(self, inputCol="Email_ID"):
        super(EmailValidator, self).__init__()
        self.setInputCol(inputCol)

    def _transform(self, df: DataFrame) -> DataFrame:
        
        @udf(BooleanType())
        def _validate_email_format(email):
            if email is None: return False
            return bool(re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$', email))
        
        @udf(StringType())
        def _extract_domain(email):
            if email is None or '@' not in email: return None
            return email.split('@')[1].lower()

        legitimate_domains = [
            'gmail.com', 'yahoo.com', 'hotmail.com', 'outlook.com', 'live.com',
            'icloud.com', 'singnet.com.sg', 'starhub.net.sg'
        ]
        disposable_domains = [
            'tempmail.com', '10minutemail.com', 'guerrillamail.com',
            'mailinator.com', 'yopmail.com'
        ]
        
        df_transformed = df.withColumn("Email_Valid_Format", _validate_email_format(col(self.getInputCol())))
        df_transformed = df_transformed.withColumn("Email_Domain", _extract_domain(col(self.getInputCol())))
        
        df_transformed = df_transformed.withColumn("Email_Domain_Legitimate", col("Email_Domain").isin(legitimate_domains))
        df_transformed = df_transformed.withColumn("Email_Disposable", col("Email_Domain").isin(disposable_domains))
        
        return df_transformed

# ==============================================================================
# 2. Main Pipeline Execution
# ==============================================================================
def main():
    # Initialize Spark Session
    spark = SparkSession.builder \
        .appName("DataProcessingPipeline") \
        .master("local[*]") \
        .getOrCreate()

    file_path = 'singapore_loan_data.csv'
    
    # Load data
    print(f"Loading data from {file_path}...")
    try:
        df = spark.read.csv(file_path, header=True, inferSchema=True)
        original_shape = (df.count(), len(df.columns))
        print(f"Successfully loaded {original_shape[0]:,} rows")
    except Exception as e:
        print(f"Error loading data: {e}")
        spark.stop()
        return

    print("=" * 50)
    print("RUNNING PYSPARK DATA PIPELINE")
    print("=" * 50)
    
    # Define column types for processing
    numeric_cols = ['Age', 'Loan_Amount_SGD', 'Outstanding_Balance_SGD', 'Day_Past_Due', 'Tenure',
                    'Interest_Rate', 'Current_EMI_SGD', 'Number_of_Past_Payments', 'Amount_Paid_Each_Month_SGD',
                    'Bounce_History', 'Credit_Score', 'Recent_Inquiries', 'Loan_Exposure_Across_Banks',
                    'Recent_Score_Change', 'Unemployeement_rate_region', 'Inflation_Rate', 'Interest_Rate_Trend',
                    'Economic_Stress_Index', 'App_Login_Frequency', 'UPI_Transactions', 'Online_Banking_Activity']
    
    categorical_cols = ['Product_Type', 'Payment_Frequency', 'Gender', 'Occupation', 'Employeement_Type']
    
    date_cols = ['Installment_Due_Date', 'Last_Payment_Date']
    
    bool_cols = ['Partial_Payment_Indicator', 'Repayment_Irregularity_Flags',
                 'Mobile_Number_Active_Status', 'Email_Activity', 'Do_Not_Call_Registry_Data']

    phone_cols = ['Primary_Phone_Number', 'Secondary_Mobile_Number', 'Landline_Phone_Number']

    # === Pipeline Stages ===
    # Stage 1: Custom Feature Engineering
    address_processor = AddressProcessor(inputCol="Address")
    phone_processor = PhoneNumberProcessor(inputCols=phone_cols)
    email_validator = EmailValidator(inputCol="Email_ID")
    boolean_converter = BooleanConverter(inputCols=bool_cols)
    income_encoder = IncomeBandEncoder(inputCol="Income_Band_SGD", outputCol="Income_Band_SGD")
    date_extractor = DateFeatureExtractor(inputCols=date_cols)
    engagement_calculator = DigitalEngagementCalculator(outputCol="Digital_Engagement_Score")

    # Stage 2: Impute missing numeric values
    numeric_imputer = Imputer(inputCols=numeric_cols + ["Income_Band_SGD", "Digital_Engagement_Score"], outputCols=numeric_cols + ["Income_Band_SGD", "Digital_Engagement_Score"], strategy="median")
    
    # Stage 3: String Indexing for categorical features
    indexers = [StringIndexer(inputCol=c, outputCol=f"{c}_index", handleInvalid="keep") for c in categorical_cols]
    
    # Stage 4: One-Hot Encoding for indexed categorical features
    encoder = OneHotEncoder(
        inputCols=[f"{c}_index" for c in categorical_cols],
        outputCols=[f"{c}_vec" for c in categorical_cols]
    )
    
    # Stage 5: Assemble all feature columns into a single vector
    final_numeric_cols = numeric_cols + ["Income_Band_SGD", "Digital_Engagement_Score"]
    feature_cols = final_numeric_cols + [f"{c}_vec" for c in categorical_cols]
    vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")

    # Stage 6: Scale the assembled feature vector
    scaler = StandardScaler(inputCol="features", outputCol="scaled_features")

    # Create the full pipeline
    pipeline = Pipeline(stages=[
        address_processor, phone_processor, email_validator,
        boolean_converter, income_encoder, date_extractor, engagement_calculator,
        numeric_imputer,
        *indexers,
        encoder,
        vector_assembler,
        scaler
    ])
    
    # Fit and transform the data
    print("Fitting and transforming data with the pipeline...")
    pipeline_model = pipeline.fit(df)
    processed_df = pipeline_model.transform(df)
    
    # Optional: Save the pipeline for later use
    pipeline_model.write().overwrite().save("pyspark_data_pipeline_model")
    print("Pipeline model saved to 'pyspark_data_pipeline_model'")

    print("\nPipeline completed successfully!")
    print(f"Original shape: {original_shape}")
    processed_shape = (processed_df.count(), len(processed_df.columns))
    print(f"Processed shape: {processed_shape}")

    # === Display Results ===
    print("\nPipeline Summary:")
    print(f"Processed data shape: ({processed_df.count()}, {len(processed_df.columns)})")

    print("\nSample of processed data (showing key new columns):")
    processed_df.select(
        'CustomerID', 'City', 'Pincode', 'Email_Valid_Format', 'Email_Domain_Legitimate', 
        'Primary_Phone_Number_Valid', 'scaled_features'
    ).show(5, truncate=False)

    print("\nSchema of the processed data:")
    processed_df.printSchema()

    # === Replicate original output summary ===
    print("\nNew columns created by data cleaning (non-null counts):")
    new_columns = ['City', 'Pincode', 'Email_Valid_Format', 'Email_Domain', 'Email_Domain_Legitimate', 'Email_Disposable']
    for col_name in new_columns:
        non_null_count = processed_df.filter(col(col_name).isNotNull()).count()
        print(f"{col_name}: {non_null_count:,} non-null values")
    
    print("\nPhone number validation results:")
    for col_name in phone_cols:
        valid_col = f'{col_name}_Valid'
        if valid_col in processed_df.columns:
            valid_count = processed_df.filter(col(valid_col) == True).count()
            total_count = processed_df.select(valid_col).count()
            print(f"{valid_col}: {valid_count:,}/{total_count:,} valid numbers")

    spark.stop()

if __name__ == "__main__":
    main()
